{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "42dcf0c0-28f0-4386-9ee3-367f3606aa30",
    "_uuid": "33595673f3f93faf28ed0ac10f0a7c0e59a9c0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as func\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import random \n",
    "\n",
    "\n",
    "# %%timeit\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "print(\"USE CUDA=\" + str (use_cuda))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "manualSeed = None\n",
    "def fixSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "if manualSeed is None:\n",
    "        manualSeed = 999\n",
    "fixSeed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "579bc4ea-49c2-4413-a4bf-57472a155db4",
    "_uuid": "ab87c9fc87053c27d96e4765be8a942e91bf79bd"
   },
   "source": [
    "### Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "796ac7f9-d66a-4856-acba-b1be8f4960b6",
    "_uuid": "e32251e2e44d9bf3b14af6153643b36110cb17ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hot_dog/115434.jpg</td>\n",
       "      <td>hot_dog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hot_dog/959883.jpg</td>\n",
       "      <td>hot_dog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot_dog/3867396.jpg</td>\n",
       "      <td>hot_dog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hot_dog/3622018.jpg</td>\n",
       "      <td>hot_dog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hot_dog/1017226.jpg</td>\n",
       "      <td>hot_dog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file category  category_id\n",
       "0   hot_dog/115434.jpg  hot_dog            0\n",
       "1   hot_dog/959883.jpg  hot_dog            0\n",
       "2  hot_dog/3867396.jpg  hot_dog            0\n",
       "3  hot_dog/3622018.jpg  hot_dog            0\n",
       "4  hot_dog/1017226.jpg  hot_dog            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoneDataset(Dataset):\n",
    "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0] # file name\n",
    "        fullname = join(self.root_dir, img_name)\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 2] # category_id\n",
    "#         print (labels)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "    \n",
    "\n",
    "import os\n",
    "data_dir = '/home/nec/Downloads/TCB_Challenge_Data/TRAIN_TEST_Data/'\n",
    "\n",
    "data_dir= '/home/nec/Downloads/dog/train/'\n",
    "\n",
    "def find_classes(fullDir):\n",
    "    classes = [d for d in os.listdir(fullDir) if os.path.isdir(os.path.join(fullDir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    num_to_class = dict(zip(range(len(classes)), classes))\n",
    "    \n",
    "    train = []\n",
    "    for index, label in enumerate(classes):\n",
    "        path = fullDir + label + '/'\n",
    "        for file in listdir(path):\n",
    "            train.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "    df = pd.DataFrame(train, columns=['file', 'category', 'category_id',]) \n",
    "\n",
    "    return classes, class_to_idx, num_to_class, df\n",
    "\n",
    "classes, class_to_idx, num_to_class, df =find_classes (data_dir )\n",
    "\n",
    "\n",
    "# class_to_idx\n",
    "# num_to_class\n",
    "df.head(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import numbers\n",
    "import math\n",
    "import torch\n",
    "import torch\n",
    "import random\n",
    "import PIL.ImageEnhance as ie\n",
    "import PIL.Image as im\n",
    "\n",
    "# adapted from https://github.com/kuangliu/pytorch-retinanet/blob/master/transform.py\n",
    "# https://github.com/mratsim/Amazon-Forest-Computer-Vision/blob/master/src/p_data_augmentation.py\n",
    "\n",
    "normalize_img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def draw(img, boxes):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(list(box), outline='red')\n",
    "    img.show()\n",
    "\n",
    "\n",
    "class Stack(object):\n",
    "\n",
    "    def __init__(self, roll=False):\n",
    "        self.roll = roll\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if img_group[0].mode == 'L':\n",
    "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
    "        elif img_group[0].mode == 'RGB':\n",
    "            if self.roll:\n",
    "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
    "            else:\n",
    "                return np.concatenate(img_group, axis=2)\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "    def __init__(self, div=True):\n",
    "        self.div = div\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
    "        else:\n",
    "            # handle PIL Image\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
    "            # put it from HWC to CHW format\n",
    "            # yikes, this transpose takes 80% of the loading time/CPU\n",
    "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        return img.float().div(255) if self.div else img.float()\n",
    "\n",
    "\n",
    "class IdentityTransform(object):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        return data\n",
    "\n",
    "class RandomErasing(object):\n",
    "    def __init__(self, EPSILON = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "        self.EPSILON = EPSILON\n",
    "        self.mean = mean\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "       \n",
    "    def __call__(self, img):\n",
    "\n",
    "        if random.uniform(0, 1) > self.EPSILON:\n",
    "            return img\n",
    "\n",
    "        for attempt in range(100):\n",
    "            area = img.size()[1] * img.size()[2]\n",
    "       \n",
    "            target_area = random.uniform(self.sl, self.sh) * area\n",
    "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
    "\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w <= img.size()[2] and h <= img.size()[1]:\n",
    "                x1 = random.randint(0, img.size()[1] - h)\n",
    "                y1 = random.randint(0, img.size()[2] - w)\n",
    "                if img.size()[0] == 3:\n",
    "                    #img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    #img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    #img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
    "                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
    "                    #img[:, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(3, h, w))\n",
    "                else:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "                    # img[0, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(1, h, w))\n",
    "                return img\n",
    "\n",
    "        return img\n",
    "\n",
    "def random_crop(img, boxes):\n",
    "    '''Crop the given PIL image to a random size and aspect ratio.\n",
    "    A crop of random size of (0.08 to 1.0) of the original size and a random\n",
    "    aspect ratio of 3/4 to 4/3 of the original aspect ratio is made.\n",
    "    Args:\n",
    "      img: (PIL.Image) image to be cropped.\n",
    "      boxes: (tensor) object boxes, sized [#ojb,4].\n",
    "    Returns:\n",
    "      img: (PIL.Image) randomly cropped image.\n",
    "      boxes: (tensor) randomly cropped boxes.\n",
    "    '''\n",
    "    success = False\n",
    "    for attempt in range(10):\n",
    "        area = img.size[0] * img.size[1]\n",
    "        target_area = random.uniform(0.56, 1.0) * area\n",
    "        aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "        w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "        h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            w, h = h, w\n",
    "\n",
    "        if w <= img.size[0] and h <= img.size[1]:\n",
    "            x = random.randint(0, img.size[0] - w)\n",
    "            y = random.randint(0, img.size[1] - h)\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "    # Fallback\n",
    "    if not success:\n",
    "        w = h = min(img.size[0], img.size[1])\n",
    "        x = (img.size[0] - w) // 2\n",
    "        y = (img.size[1] - h) // 2\n",
    "\n",
    "    img = img.crop((x, y, x+w, y+h))\n",
    "    boxes -= torch.Tensor([x,y,x,y])\n",
    "    boxes[:,0::2].clamp_(min=0, max=w-1)\n",
    "    boxes[:,1::2].clamp_(min=0, max=h-1)\n",
    "    return img, boxes\n",
    "\n",
    "\n",
    "class Lighting(object):\n",
    "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
    "\n",
    "    def __init__(self, alphastd, eigval, eigvec):\n",
    "        self.alphastd = alphastd\n",
    "        self.eigval = eigval\n",
    "        self.eigvec = eigvec\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.alphastd == 0:\n",
    "            return img\n",
    "\n",
    "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
    "        rgb = self.eigvec.type_as(img).clone() \\\n",
    "            .mul(alpha.view(1, 3).expand(3, 3)) \\\n",
    "            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n",
    "            .sum(1).squeeze()\n",
    "\n",
    "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
    "\n",
    "\n",
    "class Grayscale(object):\n",
    "    def __call__(self, img):\n",
    "        gs = img.clone()\n",
    "        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
    "        gs[1].copy_(gs[0])\n",
    "        gs[2].copy_(gs[0])\n",
    "        return gs\n",
    "\n",
    "\n",
    "class Saturation(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = Grayscale()(img)\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class Brightness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = img.new().resize_as_(img).zero_()\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class Contrast(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = Grayscale()(img)\n",
    "        gs.fill_(gs.mean())\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class RandomOrder(object):\n",
    "    \"\"\" Composes several transforms together in random order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.transforms is None:\n",
    "            return img\n",
    "        order = torch.randperm(len(self.transforms))\n",
    "        for i in order:\n",
    "            img = self.transforms[i](img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class ColorJitter(RandomOrder):\n",
    "    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n",
    "        self.transforms = []\n",
    "        if brightness != 0:\n",
    "            self.transforms.append(Brightness(brightness))\n",
    "        if contrast != 0:\n",
    "            self.transforms.append(Contrast(contrast))\n",
    "        if saturation != 0:\n",
    "            self.transforms.append(Saturation(saturation))\n",
    "\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips the given PIL.Image with a probability of 0.25 horizontal,\n",
    "                                                                0.25 vertical,\n",
    "                                                                0.5 as is\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        dispatcher = {\n",
    "            0: img,\n",
    "            1: img,\n",
    "            2: img.transpose(im.FLIP_LEFT_RIGHT),\n",
    "            3: img.transpose(im.FLIP_TOP_BOTTOM)\n",
    "        }\n",
    "\n",
    "        return dispatcher[random.randint(0, 3)]  # randint is inclusive\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    \"\"\"Randomly rotate the given PIL.Image with a probability of 1/6 90°,\n",
    "                                                                 1/6 180°,\n",
    "                                                                 1/6 270°,\n",
    "                                                                 1/2 as is\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        dispatcher = {\n",
    "            0: img,\n",
    "            1: img,\n",
    "            2: img,\n",
    "            3: img.transpose(im.ROTATE_90),\n",
    "            4: img.transpose(im.ROTATE_180),\n",
    "            5: img.transpose(im.ROTATE_270)\n",
    "        }\n",
    "\n",
    "        return dispatcher[random.randint(0, 5)]  # randint is inclusive\n",
    "\n",
    "\n",
    "class PILColorBalance(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Color(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILContrast(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Contrast(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILBrightness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Brightness(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILSharpness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Sharpness(img).enhance(alpha)\n",
    "\n",
    "\n",
    "# Check ImageEnhancer effect: https://www.youtube.com/watch?v=_7iDTpTop04\n",
    "# Not documented but all enhancements can go beyond 1.0 to 2\n",
    "# Image must be RGB\n",
    "# Use Pillow-SIMD because Pillow is too slow\n",
    "class PowerPIL(RandomOrder):\n",
    "    def __init__(self, rotate=True,\n",
    "                 flip=True,\n",
    "                 colorbalance=0.4,\n",
    "                 contrast=0.4,\n",
    "                 brightness=0.4,\n",
    "                 sharpness=0.4):\n",
    "        self.transforms = []\n",
    "        if rotate:\n",
    "            self.transforms.append(RandomRotate())\n",
    "        if flip:\n",
    "            self.transforms.append(RandomFlip())\n",
    "        if brightness != 0:\n",
    "            self.transforms.append(PILBrightness(brightness))\n",
    "        if contrast != 0:\n",
    "            self.transforms.append(PILContrast(contrast))\n",
    "        if colorbalance != 0:\n",
    "            self.transforms.append(PILColorBalance(colorbalance))\n",
    "        if sharpness != 0:\n",
    "            self.transforms.append(PILSharpness(sharpness))\n",
    "\n",
    "def default_loader(input_path):\n",
    "    input_image = (Image.open(input_path)).convert('RGB')\n",
    "    return input_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc8a9969-280a-4ec8-850b-25aed1ee38d6",
    "_uuid": "0163fcd2a2ea5a4e93bc87f47a96f404bcad6a83"
   },
   "source": [
    "### Setup transforms, datasets, and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f94cb9fa-e76a-46d5-a363-8856b45c59e1",
    "_uuid": "fe82da4f8b1501203d12027200d1f8d2209f0057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid': 75, 'train': 423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:397: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.RandomSizedCrop(image_size),\n",
    "    PowerPIL(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "valid_trans = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_data = df.sample(frac=0.85)\n",
    "valid_data = df[~df['file'].isin(train_data['file'])]\n",
    "\n",
    "train_set = BoneDataset(train_data, data_dir, transform = train_trans)\n",
    "valid_set = BoneDataset(valid_data, data_dir, transform = valid_trans)\n",
    "        \n",
    "\n",
    "t_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "v_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(t_loader.dataset), \n",
    "    'valid': len(v_loader.dataset)\n",
    "}\n",
    "\n",
    "print (dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d966c8a9-d189-4b76-8def-6180f9498154",
    "_uuid": "fde155bdd9ce81e2598146c263cedfa65eaba806"
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f2eb7b14-c63b-4fdf-8370-baabd48a9943",
    "_uuid": "29184efaeb75c7105b9f144550b68814c577534a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.30)\n",
    "relu=torch.nn.LeakyReLU()\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "class ConvRes(nn.Module):\n",
    "    def __init__(self,insize, outsize):\n",
    "        super(ConvRes, self).__init__()\n",
    "        drate = .3\n",
    "        self.math = nn.Sequential(\n",
    "                 nn.BatchNorm2d(insize),\n",
    "                 nn.Dropout(drate),\n",
    "                 torch.nn.Conv2d(insize, outsize, kernel_size=2,padding=2),\n",
    "                 nn.PReLU(),\n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.math(x) \n",
    "\n",
    "class ConvCNN(nn.Module):\n",
    "    def __init__(self,insize, outsize, kernel_size=7, padding=2, pool=2, avg=True):\n",
    "        super(ConvCNN, self).__init__()\n",
    "        self.avg=avg\n",
    "        self.math = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=kernel_size,padding=padding),\n",
    "            torch.nn.BatchNorm2d(outsize),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(pool,pool),\n",
    "        )\n",
    "        self.avgpool=torch.nn.AvgPool2d(pool,pool)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.math(x)\n",
    "        if self.avg is True:\n",
    "            x=self.avgpool(x)\n",
    "        return x   \n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.cnn1 = ConvCNN (3,64,  kernel_size=7, pool=4, avg=False)\n",
    "        self.cnn2 = ConvCNN (64,64, kernel_size=5, pool=2, avg=True)\n",
    "        self.cnn3 = ConvCNN (64,256, kernel_size=5, pool=2, avg=True)\n",
    "        \n",
    "        self.res1 = ConvRes (256,64)\n",
    "        \n",
    "        self.features = nn.Sequential( \n",
    "            self.cnn1,dropout,          \n",
    "            self.cnn2,\n",
    "            self.cnn3,\n",
    "            self.res1,\n",
    "        )        \n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(2304, len(classes)),             \n",
    "        )\n",
    "#         self.sig=nn.Sigmoid()        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x) \n",
    "        x = x.view(x.size(0), -1)        \n",
    "#         print (x.data.shape)\n",
    "        x = self.classifier(x)                \n",
    "#         x = self.sig(x)\n",
    "        return x        \n",
    "\n",
    "model = Net()\n",
    "    \n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False    \n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = torch.nn.Linear(num_ftrs, len(classes))\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.00005 * 2 * 2)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0cd8c571-5d93-42b6-9ef0-9c16b6d43ef7",
    "_uuid": "1257d2cc10e64019a8ca94c814d4e179a45e04cd"
   },
   "source": [
    "### Training\n",
    "mostly if not entirely from pytorch transfer learning tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f2cd5d63-c765-476a-9258-0152d8a06360",
    "_uuid": "d5ecfa57978ed8afd52e1551f6f5688ce9e16a5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "\n",
    "def train(train_loader, model, epoch, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in ((enumerate(train_loader))):\n",
    "        correct = 0\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "        accuracy = 100. * correct / len(data)\n",
    "        optimizer.step()\n",
    "#         if batch_idx %500 == 0:\n",
    "#             print('TRAIN: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.data[0],\n",
    "#                 correct, len(data),\n",
    "#                 accuracy))            \n",
    "\n",
    "\n",
    "def test(test_loader, model, epoch):\n",
    "#     model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in (test_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nVAL: Average loss: {:.6f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b939262-bef9-4384-9e65-1c6f19b0e7af",
    "_uuid": "c89aec6e431baa5ad878aa07c30faef161dea697"
   },
   "source": [
    "### Train the model\n",
    "using one epoch due to time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c75d0756-757e-4cdb-b3e3-43d0ae2110eb",
    "_uuid": "56b469e006382a0c93c7ce30b7976783257762b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:04<02:20,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.634824, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [00:08<02:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.666359, Accuracy: 48/75 (64.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [00:12<01:56,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.655883, Accuracy: 49/75 (65.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [00:16<01:50,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.609540, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [00:21<01:45,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.614831, Accuracy: 54/75 (72.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [00:25<01:40,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.595556, Accuracy: 46/75 (61.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [00:29<01:37,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.677518, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 8/30 [00:34<01:33,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.623917, Accuracy: 50/75 (66.667%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 9/30 [00:38<01:30,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.598180, Accuracy: 49/75 (65.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 10/30 [00:43<01:26,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.615905, Accuracy: 51/75 (68.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 11/30 [00:47<01:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.677453, Accuracy: 50/75 (66.667%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 12/30 [00:51<01:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.607227, Accuracy: 51/75 (68.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 13/30 [00:55<01:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.600594, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 14/30 [01:00<01:08,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.652949, Accuracy: 51/75 (68.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [01:04<01:04,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.612560, Accuracy: 54/75 (72.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 16/30 [01:08<00:59,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.611997, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 17/30 [01:12<00:55,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.608953, Accuracy: 54/75 (72.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 18/30 [01:16<00:51,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.545303, Accuracy: 55/75 (73.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 19/30 [01:20<00:46,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.548123, Accuracy: 54/75 (72.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 20/30 [01:25<00:42,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.536311, Accuracy: 51/75 (68.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 21/30 [01:29<00:38,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.552508, Accuracy: 56/75 (74.667%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 22/30 [01:34<00:34,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.619999, Accuracy: 50/75 (66.667%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 23/30 [01:38<00:29,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.601757, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 24/30 [01:42<00:25,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.548447, Accuracy: 55/75 (73.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 25/30 [01:46<00:21,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.555491, Accuracy: 57/75 (76.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 26/30 [01:50<00:17,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.590802, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 27/30 [01:55<00:12,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.589349, Accuracy: 52/75 (69.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 28/30 [01:59<00:08,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.548534, Accuracy: 53/75 (70.667%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 29/30 [02:04<00:04,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.560503, Accuracy: 57/75 (76.000%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 30/30 [02:08<00:00,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VAL: Average loss: 0.547618, Accuracy: 58/75 (77.333%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "# sample_submission.columns = ['file', 'species']\n",
    "# # sample_submission['category_id'] = 0\n",
    "# sample_submission.head(3)\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    for epoch in tqdm(range(0, 30)):        \n",
    "        train(t_loader, model, epoch, optimizer)\n",
    "        test_loss, accuracy= test(v_loader, model, epoch)\n",
    "        if float(accuracy) > float(90.0): \n",
    "            print ('Is nice ...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), str(type(model).__name__) + '_' + str(accuracy) + '_.pth')\n",
    "# model = SimpleNet()\n",
    "# .. to load your previously training model:\n",
    "# model.load_state_dict(torch.load('simplenet_cnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, participants will be asked to provide the following classification rates:\n",
    "\n",
    "-- TP (True Positive, which is the number of OP people correctly identified),\n",
    "\n",
    "-- FP (False Positive, which is the number of CT people incorrectly identified),\n",
    "\n",
    "-- TN (True Negative, which is the number of CT people correctly identified),\n",
    "\n",
    "-- FN (False Negative, which is the number of OP people incorrectly identified),\n",
    "\n",
    "-- Sn (True positive rate or sensitivity) as Sn = TP/(TP + FN),\n",
    "\n",
    "-- Sp (Specificity or True Negative Rate) as Sp = TN/(FP + TN)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
