{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tel-Aviv Deep Learning Boot-camp: 12 Applied Deep Learning Labs\n",
    "\n",
    "## Lab 0: Plant Seedlings Classification (PyTorch): The most basic lab :)  \n",
    "\n",
    "<img src=\"../assets/seedlings.png\" align=\"center\">\n",
    "\n",
    "### Instructors:\n",
    "\n",
    "- Shlomo Kashani: shlomo@bayesian.io ,\n",
    "- Nathaniel Shimoni nathaniel.shimoni@grid4c.com \n",
    "\n",
    "<img src=\"../assets/pt.jpg\" width=\"35%\" align=\"center\">\n",
    "\n",
    "## Progress\n",
    "\n",
    "- [x] PyTorch DataSet\n",
    "- [x] PyTorch DataLoader\n",
    "- [x] Augmentations\n",
    "- [x] Simple CNN\n",
    "- [x] Training + train test split\n",
    "- [x] TensorBoard Support from PyTorch\n",
    "- [x] Accuray and Log Loss\n",
    "- [x] Tqdm progress\n",
    "- [x] Persisting the model\n",
    "- [x] Testing on a test set\n",
    "\n",
    "\n",
    "\n",
    "### Links:\n",
    "\n",
    "- https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/ \n",
    "- Git: https://github.com/bayesianio/applied-dl-2018\n",
    "- Full info: https://www.evernote.com/shard/s341/sh/3855640e-2b0b-42e5-b5b9-00216d02ac9a/b47968226e49a81ee813901cd41d3924\n",
    "\n",
    "### Date and Location: \n",
    "- July 2018\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "- Python 3.5, CUDA 9, cuDNN 7, PyTorch 2.0 or above, Keras 2 or above\n",
    "\n",
    "#### For Windows 10 and Windows Server 2016, CUDA 9\n",
    "`conda install -c peterjc123 pytorch cuda90`\n",
    "\n",
    "\n",
    "### Data\n",
    "- Download: https://www.kaggle.com/c/plant-seedlings-classification\n",
    "\n",
    "- Please make sure you have already set up a Pytorch tree structure of your dataset:\n",
    "- `data_dir= '/home/data/bone/train/' `\n",
    "\n",
    "```\n",
    "    data_dir= '/home/data/bone/train/\n",
    "    \n",
    "    ├── valid\n",
    "    │   └── Type_1\n",
    "        ├── Type_2\n",
    "        └── Type_3\n",
    "    └── train\n",
    "        ├── Type_1\n",
    "        ├── Type_2\n",
    "        └── Type_3\n",
    "```\n",
    "\n",
    "### PyTorch Datasets\n",
    "\n",
    "To create a dataset, we subclass Dataset and define a constructor, a `__len__` method, and a `__getitem__` method. \n",
    "Here is full example:\n",
    "\n",
    "```python\n",
    "class BoneDataset(Dataset):\n",
    "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0] # file name\n",
    "        fullname = join(self.root_dir, img_name)\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 2] # category_id\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "```\n",
    "\n",
    "### The PyTorch DataLoader Class¶\n",
    "- Will load our BoneDataset\n",
    "- Can be regarded as a list (or iterator, technically).\n",
    "- Each time it is invoked will provide a minibatch of (img, label) pairs.\n",
    "\n",
    "\n",
    "### Training with TensorBoard\n",
    "\n",
    "With the aid of [Crayon](https://github.com/torrvision/crayon),\n",
    "we can access the visualisation power of TensorBoard for any \n",
    "deep learning framework.\n",
    "\n",
    "To use the TensorBoard, install Crayon (https://github.com/torrvision/crayon)\n",
    "and set `use_tensorboard = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "42dcf0c0-28f0-4386-9ee3-367f3606aa30",
    "_uuid": "33595673f3f93faf28ed0ac10f0a7c0e59a9c0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "__pyTorch VERSION: 0.3.1.post2\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017\n",
      "Cuda compilation tools, release 9.0, V9.0.176\n",
      "__CUDNN VERSION: 7003\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n",
      "USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "%reset -f \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as func\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import random \n",
    "\n",
    "\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "print(\"USE CUDA=\" + str (use_cuda))\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "manualSeed = None\n",
    "def fixSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "if manualSeed is None:\n",
    "        manualSeed = 999\n",
    "fixSeed(manualSeed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "579bc4ea-49c2-4413-a4bf-57472a155db4",
    "_uuid": "ab87c9fc87053c27d96e4765be8a942e91bf79bd"
   },
   "source": [
    "### Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "796ac7f9-d66a-4856-acba-b1be8f4960b6",
    "_uuid": "e32251e2e44d9bf3b14af6153643b36110cb17ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black-grass/0050f38b3.png</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black-grass/0183fdf68.png</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black-grass/0260cffa8.png</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black-grass/05eedce4d.png</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black-grass/075d004bc.png</td>\n",
       "      <td>Black-grass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file     category  category_id\n",
       "0  Black-grass/0050f38b3.png  Black-grass            0\n",
       "1  Black-grass/0183fdf68.png  Black-grass            0\n",
       "2  Black-grass/0260cffa8.png  Black-grass            0\n",
       "3  Black-grass/05eedce4d.png  Black-grass            0\n",
       "4  Black-grass/075d004bc.png  Black-grass            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoneDataset(Dataset):\n",
    "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0] # file name\n",
    "        fullname = join(self.root_dir, img_name)\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 2] # category_id\n",
    "#         print (labels)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "    \n",
    "\n",
    "import os\n",
    "\n",
    "dataset='seedings' # bone , cat-dog   \n",
    "\n",
    "data_dir= 'd:/db/data/bone/train/'\n",
    "data_dir= 'd:/db/data/cat-dog/train/'\n",
    "\n",
    "data_dir= 'd:/db/data/' + dataset + '/train/' # data_dir= 'd:/db/data/seedings/train/'\n",
    "\n",
    "def find_classes(fullDir):\n",
    "    classes = [d for d in os.listdir(fullDir) if os.path.isdir(os.path.join(fullDir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    num_to_class = dict(zip(range(len(classes)), classes))\n",
    "    \n",
    "    train = []\n",
    "    for index, label in enumerate(classes):\n",
    "        path = fullDir + label + '/'\n",
    "        for file in listdir(path):\n",
    "            train.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "    df = pd.DataFrame(train, columns=['file', 'category', 'category_id',]) \n",
    "\n",
    "    return classes, class_to_idx, num_to_class, df\n",
    "\n",
    "classes, class_to_idx, num_to_class, df =find_classes (data_dir )\n",
    "\n",
    "\n",
    "# class_to_idx\n",
    "# num_to_class\n",
    "df.head(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation \n",
    "- Many of the code snippts here were adapted from various github repos.\n",
    "- If you dont need augementation, just skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import numbers\n",
    "import math\n",
    "import torch\n",
    "import torch\n",
    "import random\n",
    "import PIL.ImageEnhance as ie\n",
    "import PIL.Image as im\n",
    "\n",
    "# adapted from https://github.com/kuangliu/pytorch-retinanet/blob/master/transform.py\n",
    "# https://github.com/mratsim/Amazon-Forest-Computer-Vision/blob/master/src/p_data_augmentation.py\n",
    "\n",
    "normalize_img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def draw(img, boxes):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(list(box), outline='red')\n",
    "    img.show()\n",
    "\n",
    "\n",
    "class Stack(object):\n",
    "\n",
    "    def __init__(self, roll=False):\n",
    "        self.roll = roll\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if img_group[0].mode == 'L':\n",
    "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
    "        elif img_group[0].mode == 'RGB':\n",
    "            if self.roll:\n",
    "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
    "            else:\n",
    "                return np.concatenate(img_group, axis=2)\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "    def __init__(self, div=True):\n",
    "        self.div = div\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
    "        else:\n",
    "            # handle PIL Image\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
    "            # put it from HWC to CHW format\n",
    "            # yikes, this transpose takes 80% of the loading time/CPU\n",
    "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        return img.float().div(255) if self.div else img.float()\n",
    "\n",
    "\n",
    "class IdentityTransform(object):\n",
    "\n",
    "    def __call__(self, data):\n",
    "        return data\n",
    "\n",
    "class RandomErasing(object):\n",
    "    def __init__(self, EPSILON = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "        self.EPSILON = EPSILON\n",
    "        self.mean = mean\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "       \n",
    "    def __call__(self, img):\n",
    "\n",
    "        if random.uniform(0, 1) > self.EPSILON:\n",
    "            return img\n",
    "\n",
    "        for attempt in range(100):\n",
    "            area = img.size()[1] * img.size()[2]\n",
    "       \n",
    "            target_area = random.uniform(self.sl, self.sh) * area\n",
    "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
    "\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if w <= img.size()[2] and h <= img.size()[1]:\n",
    "                x1 = random.randint(0, img.size()[1] - h)\n",
    "                y1 = random.randint(0, img.size()[2] - w)\n",
    "                if img.size()[0] == 3:\n",
    "                    #img[0, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    #img[1, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    #img[2, x1:x1+h, y1:y1+w] = random.uniform(0, 1)\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
    "                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
    "                    #img[:, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(3, h, w))\n",
    "                else:\n",
    "                    img[0, x1:x1+h, y1:y1+w] = self.mean[1]\n",
    "                    # img[0, x1:x1+h, y1:y1+w] = torch.from_numpy(np.random.rand(1, h, w))\n",
    "                return img\n",
    "\n",
    "        return img\n",
    "\n",
    "def random_crop(img, boxes):\n",
    "    '''Crop the given PIL image to a random size and aspect ratio.\n",
    "    A crop of random size of (0.08 to 1.0) of the original size and a random\n",
    "    aspect ratio of 3/4 to 4/3 of the original aspect ratio is made.\n",
    "    Args:\n",
    "      img: (PIL.Image) image to be cropped.\n",
    "      boxes: (tensor) object boxes, sized [#ojb,4].\n",
    "    Returns:\n",
    "      img: (PIL.Image) randomly cropped image.\n",
    "      boxes: (tensor) randomly cropped boxes.\n",
    "    '''\n",
    "    success = False\n",
    "    for attempt in range(10):\n",
    "        area = img.size[0] * img.size[1]\n",
    "        target_area = random.uniform(0.56, 1.0) * area\n",
    "        aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
    "\n",
    "        w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "        h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            w, h = h, w\n",
    "\n",
    "        if w <= img.size[0] and h <= img.size[1]:\n",
    "            x = random.randint(0, img.size[0] - w)\n",
    "            y = random.randint(0, img.size[1] - h)\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "    # Fallback\n",
    "    if not success:\n",
    "        w = h = min(img.size[0], img.size[1])\n",
    "        x = (img.size[0] - w) // 2\n",
    "        y = (img.size[1] - h) // 2\n",
    "\n",
    "    img = img.crop((x, y, x+w, y+h))\n",
    "    boxes -= torch.Tensor([x,y,x,y])\n",
    "    boxes[:,0::2].clamp_(min=0, max=w-1)\n",
    "    boxes[:,1::2].clamp_(min=0, max=h-1)\n",
    "    return img, boxes\n",
    "\n",
    "\n",
    "class Lighting(object):\n",
    "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
    "\n",
    "    def __init__(self, alphastd, eigval, eigvec):\n",
    "        self.alphastd = alphastd\n",
    "        self.eigval = eigval\n",
    "        self.eigvec = eigvec\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.alphastd == 0:\n",
    "            return img\n",
    "\n",
    "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
    "        rgb = self.eigvec.type_as(img).clone() \\\n",
    "            .mul(alpha.view(1, 3).expand(3, 3)) \\\n",
    "            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n",
    "            .sum(1).squeeze()\n",
    "\n",
    "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
    "\n",
    "\n",
    "class Grayscale(object):\n",
    "    def __call__(self, img):\n",
    "        gs = img.clone()\n",
    "        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
    "        gs[1].copy_(gs[0])\n",
    "        gs[2].copy_(gs[0])\n",
    "        return gs\n",
    "\n",
    "\n",
    "class Saturation(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = Grayscale()(img)\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class Brightness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = img.new().resize_as_(img).zero_()\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class Contrast(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        gs = Grayscale()(img)\n",
    "        gs.fill_(gs.mean())\n",
    "        alpha = random.uniform(0, self.var)\n",
    "        return img.lerp(gs, alpha)\n",
    "\n",
    "\n",
    "class RandomOrder(object):\n",
    "    \"\"\" Composes several transforms together in random order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.transforms is None:\n",
    "            return img\n",
    "        order = torch.randperm(len(self.transforms))\n",
    "        for i in order:\n",
    "            img = self.transforms[i](img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class ColorJitter(RandomOrder):\n",
    "    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n",
    "        self.transforms = []\n",
    "        if brightness != 0:\n",
    "            self.transforms.append(Brightness(brightness))\n",
    "        if contrast != 0:\n",
    "            self.transforms.append(Contrast(contrast))\n",
    "        if saturation != 0:\n",
    "            self.transforms.append(Saturation(saturation))\n",
    "\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips the given PIL.Image with a probability of 0.25 horizontal,\n",
    "                                                                0.25 vertical,\n",
    "                                                                0.5 as is\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        dispatcher = {\n",
    "            0: img,\n",
    "            1: img,\n",
    "            2: img.transpose(im.FLIP_LEFT_RIGHT),\n",
    "            3: img.transpose(im.FLIP_TOP_BOTTOM)\n",
    "        }\n",
    "\n",
    "        return dispatcher[random.randint(0, 3)]  # randint is inclusive\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    \"\"\"Randomly rotate the given PIL.Image with a probability of 1/6 90°,\n",
    "                                                                 1/6 180°,\n",
    "                                                                 1/6 270°,\n",
    "                                                                 1/2 as is\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        dispatcher = {\n",
    "            0: img,\n",
    "            1: img,\n",
    "            2: img,\n",
    "            3: img.transpose(im.ROTATE_90),\n",
    "            4: img.transpose(im.ROTATE_180),\n",
    "            5: img.transpose(im.ROTATE_270)\n",
    "        }\n",
    "\n",
    "        return dispatcher[random.randint(0, 5)]  # randint is inclusive\n",
    "\n",
    "\n",
    "class PILColorBalance(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Color(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILContrast(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Contrast(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILBrightness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Brightness(img).enhance(alpha)\n",
    "\n",
    "\n",
    "class PILSharpness(object):\n",
    "    def __init__(self, var):\n",
    "        self.var = var\n",
    "\n",
    "    def __call__(self, img):\n",
    "        alpha = random.uniform(1 - self.var, 1 + self.var)\n",
    "        return ie.Sharpness(img).enhance(alpha)\n",
    "\n",
    "\n",
    "# Check ImageEnhancer effect: https://www.youtube.com/watch?v=_7iDTpTop04\n",
    "# Not documented but all enhancements can go beyond 1.0 to 2\n",
    "# Image must be RGB\n",
    "# Use Pillow-SIMD because Pillow is too slow\n",
    "class PowerPIL(RandomOrder):\n",
    "    def __init__(self, rotate=True,\n",
    "                 flip=True,\n",
    "                 colorbalance=0.4,\n",
    "                 contrast=0.4,\n",
    "                 brightness=0.4,\n",
    "                 sharpness=0.4):\n",
    "        self.transforms = []\n",
    "        if rotate:\n",
    "            self.transforms.append(RandomRotate())\n",
    "        if flip:\n",
    "            self.transforms.append(RandomFlip())\n",
    "        if brightness != 0:\n",
    "            self.transforms.append(PILBrightness(brightness))\n",
    "        if contrast != 0:\n",
    "            self.transforms.append(PILContrast(contrast))\n",
    "        if colorbalance != 0:\n",
    "            self.transforms.append(PILColorBalance(colorbalance))\n",
    "        if sharpness != 0:\n",
    "            self.transforms.append(PILSharpness(sharpness))\n",
    "\n",
    "def default_loader(input_path):\n",
    "    input_image = (Image.open(input_path)).convert('RGB')\n",
    "    return input_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc8a9969-280a-4ec8-850b-25aed1ee38d6",
    "_uuid": "0163fcd2a2ea5a4e93bc87f47a96f404bcad6a83"
   },
   "source": [
    "## Setup transforms, datasets, and dataloaders\n",
    "\n",
    "- Data loaders spit out data from a dataset in batches. This is what you actually feed the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f94cb9fa-e76a-46d5-a363-8856b45c59e1",
    "_uuid": "fe82da4f8b1501203d12027200d1f8d2209f0057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4038, 'valid': 712}\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "\n",
    "normalize_img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.RandomSizedCrop(image_size),\n",
    "    PowerPIL(),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize_img,\n",
    "    RandomErasing()\n",
    "])\n",
    "\n",
    "## Normalization only for validation and test\n",
    "valid_trans = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize_img\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "train_data = df.sample(frac=0.85)\n",
    "valid_data = df[~df['file'].isin(train_data['file'])]\n",
    "\n",
    "train_set = BoneDataset(train_data, data_dir, transform = train_trans)\n",
    "valid_set = BoneDataset(valid_data, data_dir, transform = valid_trans)\n",
    "        \n",
    "\n",
    "t_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "v_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(t_loader.dataset), \n",
    "    'valid': len(v_loader.dataset)\n",
    "}\n",
    "\n",
    "print (dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0: \n",
      "i=1: \n",
      "i=2: \n",
      "i=3: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAABvCAYAAACjFLT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvWusLVt23/UbY86qWmvvc1/dbbex\nndghBIwIIcEEAgQlHzBWEA6ICAkwCCLxkFEiRQoRCMXCQZaCUD5YisBCwsjIRkmQCCEYAVFiJRBQ\nQoIDFiEPIcWRSezY/biPc/Zeq2qOMfgwZtVae59zu/fpvqfPun330D13r0dVraqaNed4/cd/SETw\nKI/yKI/yKI/yukVf9wk8yqM8yqM8yqPAo0J6lEd5lEd5lAuRR4X0KI/yKI/yKBchjwrpUR7lUR7l\nUS5CHhXSozzKozzKo1yEPCqkR3mUR3mUR7kIeVRIZyIiPyAiP/66z+NRXiyP43PZ8jg+ly0fh/G5\nCIUkIr9eRP43EXlPRL4gIv+riPza131eX6mIyP8uIr9CRP52Efmpe999SkT+GxF5JiJ/XUT+xdd1\nng+VT9j4/DYR+fMichSRH31Np/hS8kkZHxGZRORH+rz5QET+goj8ptd5rg+RT8r49O9+XER+TkTe\nF5G/KiL/2ssc+7UrJBF5E/gJ4PcDnwK+Bfg9wPF1ntdXKiIyAN8G/L/AdwI/dW+T/xiYgc8C3wv8\nsIj8PV/Tk3wJ+QSOz98EfhD4z7/Gp/YVySdsfCrws8BvAN4Cvh/4r0Tk27+2Z/lw+YSND8DvBb49\nIt4EfjPwgyLynQ89/mtXSMDfCRARfyAiLCJuI+KPRcRPA4jILxeRnxSRz4vI50TkvxSRt9edReRn\nROR3ichPd6/jR0TksyLyP3Qr6o+LyDt9228XkRCRf0NE/mbX5L/zw05MRH5dt2zeFZH/S0R+4wOu\n51cC/08kBcY/wNmAicg18FuA74+IpxHxp4E/CvzLL33XvnbyiRmffp1/OCL+CPD5l71Rr0k+MeMT\nEc8i4gci4mciwiPiJ4C/Ri6MlyqfmPHp1/kXI2JVttH//fIH362IeK3/gDfJyf9fAL8JeOfe938H\n8F3ABHwD8D8DP3T2/c8Af4b0OL4F+IV+k35N3+cngX+/b/vt/Qb9AeAa+HuBXwT+8f79DwA/3l9/\nSz+vf5JU3N/V33/Dh1zHbwXeBW6AQ3/dgA/661/Wz+n23n7/NvDfve5xeBwfftm97X8Q+NHXff8f\nx+fF49P3+Wzf9jte9zg8js9pfID/pG8X/VyfPPh+ve4B6xfwdwM/Cvx//SL/KPDZD9n2nwH+wr0B\n+96z9/818MNn73878EfuDdh3nH3/HwE/8oIB+3eAH7v32/8T8K98mWv5X4BfDfxS4P8E5Oy7fwz4\n+Xvb/+vAn3zdY/A4Ps9t97FQSJ/g8RmAPw78p6/7/j+Ozwu3K8CvB343MDz0Xl1CyI6I+EsR8a9G\nxLeSLuE3Az8EICLfKCJ/UET+hoi8D/w48Jl7h/hbZ69vX/D+yb3tf/bs9V/vv3dfvg3457o7+66I\nvEve4L/t/oaSQIV3ReQ94B8B/iTwV4C/C/iiiPyOvulT0mI6lzdJK+Ni5RM0Ph9L+aSNj4go8GNk\nLva3veC3L0o+aePTr9kiUxLfCnzfC37/hXIRCulcIuIvk9bEr+wf/V5S6/+qyETZvwTIV/kzv+Ts\n9S8lE9n35WdJC+Lts3/XEfEfvuCcvxARbwP/JvCf9df/I/A9fb8f6pv+VaCKyK842/3vA/7iV3k9\nXzP5Oh+fj718vY+PiAjwI2QI67dExPJVXsvXVL7ex+cFUnmJHNJrV0gi8h0i8jtF5Fv7+18C/Atk\n3BTgDdKzeFdEvgX4XR/Bz36/iFxJott+K/CHXrDNjwPfIyLfLSJFRHYi8hvX8/wQOUed/Brg/zj/\nMiKeAX8Y+A9E5FpE/lHgnyatvYuUT9L4AIhIFZEdGXJYj1u/ust5dfJJGx/gh8kQ2PdExO1XcQ1f\nE/kkjU/39v55EXnSj/nd5LX+5ENP/LUrJDJc9Q8Bf1ZEnpED9X8DKzrk9wB/P/Ae8N+TC/pXK3+K\nhC3+CeD3RcQfu79BRPwsqSz+PTIx+LPkw/Kl7tl3Aj8lIp8GLCK++IJt/i1gTyYn/wDwfRFxyR7S\nJ218fjcZBvl3SWv1tn92qfKJGR8R+TbSSv/VwM+LyNP+73s/gmt6VfKJGR/S0/s+Mlf2ReD3Ab8j\nIv7bh5649ATUJ0Ik6xX+Gplka6/3bB7lvjyOz2XL4/hctnw9jM8leEiP8iiP8iiP8iiPCulRHuVR\nHuVRLkM+USG7R3mUR3mUR7lcefSQHuVRHuVRHuUi5FEhPcqjPMqjPMpFyEXUV7wlEnXY93IwIYB5\nnjnSEPLja0Z2b1zz5jtvc3s80gKGMjCMhWVeuH32lDfeeMLt7VOm3cDV1UiY8WRfefutJ3zjN34j\ny2wcjzMfvPeU+XbhuDREhJv5wOc//0Vuj85785FJCgOKh1N3O24Oz7geJyqNqRrf/E1vcX09UKTR\n2sJ+/w6f+ey38Nbb7yDu+DIzjANlLMxtwT3wCIrAB+/+AsebZxStVJlAFTP4sZ/86a+2GO6VyW/+\n7t8QAKfwroAIKoAIROI9rRkenm8EtBRUddvFzXF3zo+VdY45xkFARH8C8jtBEFVEQNZ6QRFkeynb\n8aR/Lvm/uxdxLzQt/bw5py45Oy9fP/O8uJ/4E3/6YsfnH/y1vy4CaEejDgIheS8U3ALCcQ+GUjFx\nJAqIEGJoKTnHHFQFItBSsLA8uMh2b2Sdn+5EBB6OIkj0W6OAymnMEVozhnFARHAzxnHEzBh04HB7\nZBxGtARSlGgOFA7zkW/4pk+xf3LFWCo2H7h97wu0ecbMaG6Y5dh4OB7On/qzP3Wx4wPw2//ZXxU5\nKSpSRzygP1pIqZRhIrx/5gHueJAfGDigAqogJd+IBiELxkKZhDIpMgHVKEPDpKF1wSUoQ86jwGmt\nEWFkeZ3mgVfpY5dkGFDuz6Nyei9UwhrmTnjDzAgPigoqStGSnwE4/P4f+MtfdowuQiHtGGgeLO5U\nLSy20DAqOVkmlGGoTMNIWxY0FA3HzDh+cMTDcOD999+naFDKDgtnGKBW5frJNeYNwRmHwn4/MB9u\nUfqgB0z7PZRGCIgq7fbAUEaiZSH4VIKrq5HrJxO7XaUqlKLUuuPJ9RVX11dI5EAKBRUlGkzDFYf5\nSLOZxZzrtz/L9RuNdjhw/OAZ7Wi0dtkIzZMiSk0TRFcgsi300ZWQ9O9DTjyJfWMgFcGqPFZJxROb\nIlsVTyoYRVX6fus5RC66mgorlWCsaiyPr6djr9/dnQ15XnE6Yl7Ren5n17Qe+VKlWwvoABJl1bNI\nHycP0CoYnjdBoxsTXTERUHIhVJFNGW1KqA+hiKZRsb4n700pQJQt3rKOb7hTq+Y5mKGi2JzHnltj\nHMc8HVE8IhfDgHGsuHCm2FIcwQJCZb1ovDmt2Su9vx+F7K72EMr+2//hB+/zc3/lpwjPRR4LGBS8\nps5wBZxAcARt4OpoAXBMIYrgoRQcs8i/+NkvKLJquS6xGoya42v3g2h3bnWDcKIfU0VxTePHmflK\nlrWLUEgFYRZhCWhmVARBaQiDKDoU6tWATIK7oyJUg7kd0QJ1KPkA4+z3I28+ueZ4eJ+pVKYh1Vo0\nQ6WgpXB9dUWbG+/bU2wRwnPilqLoqCzHmWcEb6nTloW3x4F33tnzxvXEWAQJw1pQysB+v+etdz5F\nLZVaB8QCC0e9UErFlkCi8OT6beo0YvMR9eDIUwoVAva769c9BF9SYluuY1UVZ5+eLfPPOSVpwW4W\nNKuSOXk1L9jtzrbaLe5VUbifn4tsys5jPbd1MXzB+bzgyk6f+vYqVs+P1UO6cIXkvl1riCEoIekp\nhQoigahQh4EIx2dHBtlGMx1TITTvwrI0kEItAg5FCx6OkcogHGoptNsjWhRpglRozdFBNyNAusdl\nrSGlECEsx4X9fsLFgUACmuUolAIuQZAkmwGERl6LVDyOIGBmuEFglKGgOryW+/4ycrV/goV/+Q3P\nRQQoWDTCYQxBi7JaUuGCo6AFt0AWCD1FKFIUq4AbRhASL5oIz0tASB5r9Zbg3DhdN/I7x1NRnBlz\nw1uu1foSmaGLUEjTbuJ2WRBgKIUCiBtCUKuyu95RBsFYgAJN0aJMw0RETkYXQwmmQaEt7GtlPwzs\n6pieC0KpFdVCRHD15AnPbmdub29oNw1FsKXB3JhUKQPs9gPRFp48GbjeF3ajQjSiBW5Qpso07nny\nxpuUOiDmIIoMFQlhWWYYBmodAaXNCxIKEkz7JyBC1ZphlUuWLeomp+d883DSW6E/6H0tWqOvfYs4\n33OTh8ZYYvPC4oWf5+ncP1r35raTOZ3L3d++6ylt/4/Ta7/oYFBX3EVSWUcQ5kiVOx6su2OtbWPk\ni8PqvTgU7eZGQKGiwzp4GW4GMHdwybkyW4ZzPHAVJECrnjyXOIsE6WmshrHmdiY4IFVQKXgPA6ax\nrZit4yCoVijaQ0wzilJ3yrIEbk5bLt9DkioU15fytWutIIFSiOLUUlLJb/NKASVcQSPXQlcwJ5p1\nb1gQ04zJemTYzh1JVyoNrjOvKbrBh3p/dhz0TCmdKdUMxfX3fv5hQSVA8jmAhyvii1BIu31hHwEs\nPLVbdqRnOI07CGc/7Vj8yGJOWCAUaLCbBtwNwpnGQtXKbqwMVakU3nzjmt1uB57hvTLA4gal4MBi\nzu1hYbHMf8S8cDUNmf8ZKoTz1nVlPxX2Q0V8oYhQxwkcap148uZbDNMuLbelET5DgAlQFC9KiGLH\nhVqEWnIitjZjRF/XL3tCndb6UyjtFIcTIrp38ZxSej5QlhLnh9vey9ls3bwoyEX2TEF86HlyTzFF\nhg4JEO0L9jY55LmdV++vZ5MIWS3Kl7RsX4OsullEoJzUq4gQIkg40TwXljXFFqk1VqNbRdLSrjkf\nStUennPCg2kYOCwzqDKMlXaMbrGvwaMMq0q/zb4YDWec0oMxXygyMjfPnNUghESGCL3vFyCRz5Gb\nU2slWNKDC2hLyzxZN4/CoZQXPWOXJaVUTOwlFdJEqKPipxyp5vqRK72Rsda+fkTJYW/WI0bk/6pA\nUyjGC5//O9bD6TPPzBWKI+UFXs59ZXRHJF3eZptB86BrfvCWr1C+4TOfgs9/nj2Ft23keLjJGKYY\noCw3z3JS9XinkWGfY1tQh1pgmArvvPkG+6uRohDW0HHANZ3/KorWIUMPETBMDNMOHQ9UnGFX0dvg\nnU+/RR0LtsyZv5pgKgW8oSjjVKl1ZBgGnrzzNuPuCvOF5k7zBUwoUjIvNA4MGaZnRJkPB6IWCEdE\nKKUCwX5/2SE7KXJvoV9Dbid3aP3WI04LOroppfM8zPmjf/76TjDgLA9xDmLg3jZwUlabzjpFovr7\n7sXRz014Th/BmTKLNa/Vc1sv+vELkgjPxYjTNVgEYy14BOZBUe33NK3dULo3lcCGzDtFuoNr/qZf\nupAKvc1HNCDMmM3SJGkZCkKFsDUPJAzDABIUSwBCHQdKNwrUAMmF1d2popSh4At9vE4rXFFYELAe\nGuoLss9LPmEC5i+zzL8eqZIa4mVMm5CM/qy50pSTIUgPbKbnGOCGLzloETWBVFiGbks3rET768j7\nvOoZ74ZF+NlcTYXnnoCIF4kAaOkh9dZ14/358vD5cxEK6bPf+g1cv/MEB4xgXhrHeWaZG8f5gC/B\n8fZAuznAtOPQZiKEWipX04AUmMaBOlWaG8d5pjRnHAcigmEoGcs24+rqCe89fR9fFvZXE2++c8Uy\nN2xeGMpEkaC4I6VQS7CbBsQaWpRx2nF1dcXVG28wjSN1qgy1Mh+PTNOOpo3MZEEZR2odqFJo1tJ7\nioa1RikDhCBUahHKi6yPC5JSNJXCqliiL17dc5Hu3a9hl+gWrpATIOXFITs5/06Cc2NqU0ovsLBO\neShIpRTdM1sDb3cVSZwpylXh3JncPbkvcspXpdWoabFfsEgoJQQL73pXqEVpZmQ829GpniEcc4GX\nKkTL0JeIJxpuKkRruXKWNJ7HWlmsZbjbAp+PRMnc3rb+dBhY/hGWxQgS1GARtK5QJAx0pEgP83UE\nJSgUJSIT8NYcCWfQihdLpCUwSiHEMwJB/iZ22REGyEfxK57lq/fynDbL8Cmq66oD0bqX1C2JUnoA\nxjPEt+WYAq/9mGvCrnvCPVvYT9gyZNifnecVkyKqCYYxfcFJvtxVX4RCevubPsVbsS5m4MuCR6HN\nC0tzlgi+8Pkv8Plf+Dw3H9xyjIVjwLwc2E/XXF+/hdRAiiPuTKWgWnj2/vu88dYb1LGwLAtit9wc\nG7udYt7YVcelcDwaZkrYjqKJ7CqkpyvqlKEyDgPXV0/Y7XbsdiOlZi7IPCO5y3JkPtwyjLuMxVt6\nePPxmNDYkp6Th1FEaW7UYaThPPn0p1/3EHxJ0aHc8ZDcPJ+7IFes0j0ZM9yddGoS+nkWeDuTNfSX\nr2X7Xk5ej8cZBPWklO7mitY8Q5yFDVl1DkJXaH0beghORNIRkLs5JTlfNSLSgJS+gF+waBHMO6TJ\nE47/T3zXw9Fclyh/7s//JcIdpwENrZLKtvQwnxulFKQIy8fAQ4po8JKGjVuiHrVUiiSE3q1D7s0y\nTKqA9zAenmi8DvdXD8INWQKGINSRCgyClrJtew7oSSncSSMEG5hI+y8FQTh0kwRHCDPcjQjbpryK\nkqvpw+QiFFKtguqICByPBzAYHKZpouyUm9Yon/kMVQqfk89xfLexHI+UWqnVEV24ut5TqjPfHlja\nwm43Me0HYnQYoVZwOeDtltmBEUQaNRqtOkUyXj7VSpGCRA6LUlCBQUfKboKiND1Z2tCQnky8utrj\nDh4KGDTBjzNlHLu3rIjnUmkWNDsgRXn3g/de493/8iK6huVyYVZycejRtExFIETJ+KT4uReyhhfu\nBOT6n1Vd5fu04CM9LfceA9ceXngetBDbseLMEIse4stzDvqusobqZEP6SfcmTt7TmTcWfdJJcOni\nrWUuqBS8vQBe+LGUXEybrSY8FAks0iEqWkCVCCPksiMMkKGw1UN9qETP7RQRSi2YB9EMt4b7qU5O\nejg2YSmplIKGtT6nLBBvqYxCUC14d2sj8v46cAeJRNmcHQdU880dZ7QDWrAMzpunQsokej+KlPTS\nHvhMXoRCMmtIgVIrLBlKoHnPwS1MCDIVePMJ2haePf0CNht1B9fXQh0bjI02NOoEtUwIjutCTIVZ\nnxIx5+LpzuF4pAwVGQtejUkVKULYzFiUq3GPHxvigoYgQ2UcplyQPWjhmDtFhQiF49zzQkoLKKIU\nqZRSMBQ3Q8g4vTfnGAthgRWw48xwc+l9xk7eiagS2pWxWeb7IzZXXkO3Arn1EYytoOfkCUHqI+Gs\nMLWHAdPLMhDJCIPInWNtBZqrMlqV5RZmjzMF1PVe/067ZyTb6Xg/tYSU56TOLL+cn/IFi9SaXl8E\ndaz4x6Au58vKOZBE6En8FC0JzHDrCLCPgQJ2m18K0ABkgalW6jBQRDG1BNFZ5g2bG0UV3MBtzZAC\nqz6IxLe0riCKI4Nm3dIgFAqyeko5odMKl9y3T+5+/vEc7iG/OAuRbxcosHpFURCTPqe+vFyEQpqm\ngWEYOLaZ1do1svpXeiKzqlAHQ8rM1ZVw9c41LY4MT2B6u1CulXEqmM+oZrHkPM8sNlNCOmJkSUz8\n4InXLwOZkjVoC0NRnMbRDHNjGBPvN9SRWz9SGSkMCMlA0Fow1ok6jkgEpVSK2VaH4TQWZmrUXKBd\nej1GXncRpU5TWnuXLLomLwPwHgbrdSbE5mVoFdxTKQXeXX3v4Ie7lvv26l4ILlgVkqPaQ2xrCE7O\n9+5K5/yjVQltHtOqmO6GCLcp1D2oNZy3er0hwVr+uym9C5bYNG6WI4hc+PP0AAkUjw6CkPT+OlCc\nQuaEKUqzYy+fv2xJVomX20dENtYD1dKh17IdLyJDl2lkRS96zX/Rw27WVs9fIFHkVF1TR5UiaWAq\nhqOo5RxyXYOAd/Dc6H0UKyfTkJAMz62JxR5Dzxn0MO/wIhQSxwWRIA43VAJDaAS+5oSurvng5ovM\n5cDw6cpn3vgMpo7uKlaMRR0bF96dv8i+TLS4QVC8OnMsDGWHRWAEVZwiJW1gcawYKkZVZSwj8/wM\nx2g0Zo6MUyV0IZGpA0Mbe06jQikZuhp2Cdf3hiiEJfS7amW8HnALqg6IKNZmRJTdfuJwe8P+yRNi\nuWymBj0zje54JRJISSUk6ZN2K0q60RYE3gEPwYpb21yUPGB6WPmnI8aieyyp/E7OT0f7SbClddZD\n9TBqwpxP3hHkeW4iuajFnUXs5AEmsOEElvgYrHWoaoZNi+Lz3fH6uMrssebZcSznmSjTbuBwOCQ8\nfQjchPYxcGOlVDTsgcty36c/pQqInv6yli80z2LmngfaYmxb6K2sJl4WEivIkGE6MU1KoujPi5R+\n8PTCtvNc599q6J0/WuUUThU6ypIeJgxl5ULSl5hEF6GQJIzjsyPrBUd4h6VCYHxw+CImDXbBWCpt\nrggNK8Gc6AEW6WG/QaFFUgUV2NWJZt6teQEtBBlWWtHAwzgRdqDRkKEgqtSxIiqEGo2g1ILbwm3c\n9hhQwaxQ5YqdNwYqRQviSXPEykdl4C5Ao5ba4bTO0m4pVYiWUNpLlpNRtIII+ts+RqJ5I7V0aHEE\nGgphWxgsPcYOdDi73jh7dULwrbHt2PJHp1Dd+YmRkFjplpp2W+0sn7TpvTv79uN2rXWOzpM7F9i3\nvvAFT5WtPq9U/7rwkHDHmmEYhV7nlMlcZFaklAzrFkU+Bii7WgfCCy9jekauPDiOuGwLPETmrfvR\nsjD2tBehiPjpEQ/tPJMNsZrLYKT3EyF47V5YTS8ugSSpCkN77lsFJbrnlOpKI702RWFlsAmHljyg\nazj/ZUbnIhQSOGVQwheaJW3JvCxpIRdocSAGYW6HnHiFrqwaUoQmLSOWEbTlyHG54Wq4JkQwnzPG\njlIo3B5u2dURN2esI7WkT4aARYMwbo9Pub5+M5WippvsZhyXhaqKyshiRw62JFKvLex0TyG9rOaK\nhDANe2wxXAzcaZFKabU4nlw94fbZLR8LM/xDpS/WPbSWOr9T10jWaIo4Tod0rwCCyDDCRnJKV0Rb\naEC3o58KbNMz2rymDbxwAjFwpjw3ZLe8KMuwhvGk1/fF2b87B+J5TXhhsilT7wnkrwfpOUUzFjPa\nvGTOxBz0ZHMXLffth4sUHQfCXrLA2hzEYEWthZGw7q4UVmYLsU4n0m/EVhHrhEkndwZrBSWICkgi\nw0uANKAKbr3IPQTrtWArHiFqcuZtiq+7biGKq6Kth+o810pvWQuXAIuHy0UopPnwDB0FHSpuzmKN\nYT8wIxzaLYflGdYaizWEDE9IFaoUVA2tux5uSXTcUN9iGq85zM+gF6AKClJTIQCEZEIQp5QhC1ZJ\n8MGb+zcZ93sOx2Mm/dxYMGSaaOFUhKUP4mK3LO0pVse0FGbY6TU7dtjxwDhMED36LQMhQS2Vtszc\nPLuhhPJSI/aa5SyFeYohryGCwhaSy8Lm7oG0ZNMIj84UvYbm4LxOIeJUkX73M2XVW2stlKpuyiYL\nWjgL7a0FsMKKqnteVu9IULVEMG1oO05hj4gXKLPLklhAB6WIdC64rwcJ5uXIsJBoO0nW6pvDTRo+\nJTroqBNTXLjUMtBekpHFbE5mhla692I9ctBOlD9BnzPpxZQzaqHmEF6SNJdMMagqJUY0BJbEQ4QE\nWa0chLSTTYbjNQ3KXoqYzgDpkVm32ySRE6xhQl+CZoEtxrIcsXbs6LsH3KeXukOvSObjIYsr54wT\nU5T5eMuzWGiy8IWbd7m6umLxRqlD1oZIFnCpKrWHiShJUdIsaHZERKhlREqi6DyM0QuH+QYPGOob\nSChmM1oFrRVrzuzOB1/8ArvdEwDCBZGRgmAEx+X2FApSsDhAOzBMO2otLNwSviBeObZblMo0XSVk\n1ZOh2Jpl7sWTy+/jI+vCfS5nOSHYCBxFM4ymJb2hdERWlUFXaGvO6Oz4Z7Hqc8h29PCa6Kpoei6p\nAxhk+/3nz2k70+2zbeuzazh/Hx96jEsTHXRLNku8HJnlpcq83DCMDlRau8XsiMXM4XjDWAdEK6gy\n6kRrl6+RVEumXF5G3AhreO2h73BOyLcTAo4NlCPIFlnw7Ss3oORTUSQ5AQu9NYQFgeHecp8zD0gg\ndY9C00x7lHrK49oKHOr7FBFECzYHZk6bYT4eOR5vsjbpAXIRCunm2TPGN3c0EaJCsyMLxpHG7I3p\n6grqgPlMRj6NqlkvZLPDBFAQHQiEcVCGWlPRaFIGRTQsjNZmrq/fwsywtlClrGAQXJRSBypK1Sm9\nZBfqsMds4TAfaMuRQSuH4y2llIzXDp3ZmwW0x7p1ZQU/EHXiVhd8EXbyhDYr7TYZwUWV9jHgSltl\n9SLOC1VXfrSeAkpvZlUoKgTa82Sx8XCFZXguthwT944pZ4Wra3it54xUe7ggJ8Yd4M9GO9NZJLb8\n06n+qJ9Zno+vXl56Q7Eu7OsFXbx/lKjGQQsh2YvmMM+v+5S+annyZGKe3+MLn/t5PBacORndB6Et\ngpYdRa9o5i+GI1+YWGu0By7K2z6WqONFwEvBzBEaRcALDBviTlJrnN0IQREN1IFau2Iy0AKmWypC\ncMwcM5IdQ1dQzEnpLZ2cUFUowwp+cEI7ri9agh7r1DscePeOFm5vjhxunz0YeHIRCumIMceRcLAG\nizeiFG7siONZz2NLD7cFtYwMtRI4u+kJSKcmKUmT38ISdaTK4kbxmosSBWHAW2RCTj0Xnq7pPRpF\nsiXE0EM6htLmGSG9Le24/2n3BLOWNVRaOoKvMtSRIgMiJBvDCEf7gMpIqHKzNNoRYhbmDwKhcvXk\njdc7AC8pa83Qtrh/WESsKwRV6YzQPc+0IQzOlNu953UNuW3+lMhJGW25obMA4h3v5sNO6rlf4JQ3\nerH0wN6XOdbrla3JoQahym68iGn9Vclx/gKLPcPtg7wuv0VLoTUYdJdtTRogUyb1L1xarGG2h4vb\nkpkgzdDbliJSqAFOsoeHAsggtirbAAAgAElEQVQ94E629QlV1Hs+h5JzL3q8TYAoYA1foC1OLZrF\nrmvOF89C3GhoTb5B1SReDWl4RK7XDnVIXlE3oy2NZXEOT2duboKHps8u4sltY95hE8e84aJYLFml\nPIwbsaIEjEPCp8MV0QEzcGvosMPI+qW0yhOZ4w4DiupEuPd8UloBZvk3EFwdw6gqYGtKEJbDAR0H\nvvOb/6mP/Lr/zJ/7Qxzem/nc3/jFj/zYr0pWQMG58xBrjdEW/VrJUM+UjWZnUXHBxTqTNnf0xilY\n1sNvnQ6oaM2wq2oP0Z1RAmE9laSbAryrYE4nuhXfvkgBdUohPZ3EFv54cQ7qcmSYClIVlYEgGOr4\nuk/pq5Zj+0WChSiHtNiL4SxgYCKoWGelcIKr1326X1bCEnH7MiJEInXdugPUn8ciQEVLItkiHM92\no+ArDZCS/CkOUhCJ3D6yzU40TTh9QGsLS2tZs1RTKeW+vdidnuduYBa4GtH7KyUpS+9gsARGdqM1\nD2yBNg94Oz4Y0HERCmmeGhA0SxoQkU4FOGQ4LauJjXHYA4Fo6cWkBfN0QxdfaM2IWBjqBBrokL1X\nlEyMrmspZ8h4EfCe7xl02mgwRGGZbynDcLcY7CMUeWNhqoVje/pKjv+1krVmZw123ZE18SlsyLu1\nDOkOg9YdPbLmqTIkVySZCNAeLdhqk7JXz9ar5TTA23HOj3n++YfZqneUj6R3dNnqCN769GcoJVCt\nBHH5hdYPkChHoC+pkeTGgsIQmB1wF6qAMp4Vb16uuNkL7aAvJWly2cbETaEDsfJbKLim0a09pXT/\nThRVktAilVb0xoZLWId9Jz3TypOnUXo3ijVyVJMjLEbA8S0qkQQPHQCO28oO0bZ8cHMhWiWWzM0/\nRC5CIR25YZD9CbIqGRMNkfRaykizhaJCQdMasMCWuTN5K7UKwzgQssuFqjXClbAF2Y3UYYe3A6XU\nLAyTkvx56hTfEShrG7mIpMSZ6jUhhZvbV8M1F0MW0r71rU9eyfE/Kok4hc+eDwW/YLmO00O7gRTo\nRJDdU1pDcMGKvrurnNZ8kWo2JZMkFtzi5OkQyWmy3vGM7tYWPUROFK939znPYV2qfOobvwEJ76Vv\nXx8KyWnduo8tp5FG+oKWCq5pjEZQPw5Q968Am56GcSLjRIWqBS1ZH1mge0dBc8tC4d7W3P30FK8I\n8fPzaHFvvplQGCjFT2wQccJPuLF1u81+WwC1q8TcvAyp+NxWMEVk/sUHYM9De75dhEIayoRFej5C\nXrzqmP3ZbSFU0HGEUrBYW1kbpfbBQrCeN6JoTspxIrr3dDjcou2WWpUw66SEqewOzz7g+uo6PSmp\nHA+3eaNFe+MwY5x2r+S6j8shG5+V/Ss5/kcla9htW7BX5EK2kry38K+QcNgYuFeVFAn4CNekBQrv\nzbsyHnsCNWTF7QZEWFF1vVXBFjZEzizGfB8k1Uw6V0khJStcaD1/hNMsXQETa9x8zYvFtu2lA6m1\nZm6gADX04gutHyISTnNnqJ2WqhOpFqkII6hS5A0k9thlE50A4C8JaICef5Vkby+ilJoUQiqa3KdG\nzh9TGk4278nndWXk915m4RHJNhOBWUtuOiSh4AVWT2ydTmsXa+9F6taZVrRI5p+KUAQ6CmIzCBWn\ntSRHDkv0hUTN8pYHyEUoJIbss0LAuJuogJTKu8/eY7fb5U2X7PdSUEQKRaFKYSXVFMkusCqVRmNX\nJ+b5lqLZ7tx6sykpFW/HjVTQAmZrlJCt0AuEZ/MttSSp4cv29HioSJ2QFszt44aKSsj1qjBe6Ils\nrNnnSRl6PokTMKEn6+4rtROIQToj6vp5bwMtnCrR7//05uv2s5WT0jn9zglH98Iw4xnw4tJltWpr\nrbTFeODcv2hRCcahQiRbg8Q6hgqtACPBiPiAXXqPebhDNvzgfWxtv5HGnbgmIKFwKgICtBTW/nvQ\nHvA7L9oiO8+eFFE3JdfHf2MJOk/8ZgxdRREtmVfquV1z66wPykai9wC5CIW0mDOOOyD71FgYxeF6\nf41qYW5HzJ2h7gizbEhG0vqU3Y6lGcMwYp1esBZJUtUiVGRLfJsDzRE6I7JUrvZvEXQ31kAiG7S9\n9eY30myhHW9ZXpEJNu72WGtou4hh+BJyjoqjh+96OE3OwAJbWC960uiExIsNBddh2kWQUDQsEUCy\nkrXK5gVlqC5RdR6Okg3cNk0SsZ3aykO3ooMSVbkqTe0Wdv9uvaZzBgc5Q9Od5aLC/eI9pE7qwrJY\nhrci+Fvveg/LnENFVm93JbBt2V/HG4f5wNwOzPMz8Bn3OccPwVoSBksZKbUyTm9muxHPcHlZe6CH\ncGyNsdZ+Xo5bNuqLFix2xDHm43uYfcDSPkfUJDdepDGUNBiGmvWEuG22oONZYyUjqldge2gToiP1\nwscHEhr9MpxuAO14hNqL/4tmC5vaso6oCtr5MUUTtIDGhp6TlfwOOnrbsgUGgWqnulnFOsfeGeAn\nC9JXwFKkUbh6Qn1OrsXvurad0YL04t2ISAAEAvpw+tuLWAm1ZvI6CMzXOGZ6Q2YLkwzobiIaHBdL\nZVQEJ5hb633mDaE3eZBCEce89xFpQZHOxIFynI+AIzU5bkPAQyk9KUwIT2+egjvidmp6+lFf97AD\njpkHu3A583My0S/n/5y4466cvJDN21m7wZ4+6h6SZtJ0pQBav+yO6coovtYkea9OX5XHhz/op/qi\nVSmdhxBFylmd09mRtmhgcg7GHbDFZYq1JV9EFoVn/i2tckS6UrDOEO2ENY7HA0u75fbmfeb5QMTM\nzIzETLLiz5jNDD1qEAi2BLXuORyvwCeWg2JNqDGy5t+lpvKn3Gs7XzKk5H6L+fuEPiVK2+DDWeie\nLQ7acszQbiTrvmhNY8QNlQGNAZER0ZEgedsuXnpvo5eRzXiIzoJqLXkiSwbHski1nPKf94IMq3Qz\nr6P0PFvOnz3X57Rep55NnYuuG3lfKkiU4cjSmSSyaWnrrevzmdPnIOkfJhehkKQqdVDakm2LC4p5\nUN3YjROgRHOsBWMZkJIWoXb3sKhSpNC8IQLWZrRWIhrNMkFnPc5znI2lBVMdOR6PjLuJ2Y4Enrmr\nDlmuZcSYQQP35RVdOGidqDq8muN/ROJx9+nelNMZK7eHb8781t48N7oDtz61c4hkcEAyB9LJIzPP\npJ009RSqc+iNEDviYT2dewWv4Xc1yMkbykJBibTu1sS/u92Zx6dK99PxL51c9dkH7xEtaT+27rkR\nmM94r5UL9/7eMLuhxTPMD0hJfrSIBbdDAgf8wDBOqGZRZLYqz5E78jQ7hcYejyuOB4PjHhggFClK\nLYVSFS89x1cEYSb8gHGDxrt5NPUOXJoREZq1bqMoGkmiJpJLVHq8FRbFXJhqwQxE4+7zebHi27P1\nUAlILIAo7podX1fAgmehrGpF1HqeqP9SQPNGyfai2fq6ZX7IvPV+Wcl0E9aLw73RzDZoeZ5ABygQ\nhGUrc7SmXtU8ryQbSEXXOu1eM2OZG8dlwU1z7f041SHR3c46FKRlv5NKdMZtpbWu5bW3AReheU66\nQQruzrP5WdZfJL02rhluqKViYWgECz0EqwOGMgwTvjjTuEuvti+utQw0W3CglEp9RR6MesUiaC/Z\nSfJrLavVdJfp4JT3OTXX87sghxcooS2XdBbSy+JZ7W2S/Qy4cPJa1lDc5kVt8kDTa6WPkBN6KUN+\n6wxfPbl+zDN6pEuvQ/r8L/xcZ7wwMiHXSFLOGY+FiCUXbl8IybyE0zA5ZK9Q6aExTYUwuIDMmSLX\nmnm7cFxaelrkax2hRuVgB6IFREEZcHOKFlSGvlg6YgdcbnBucZlRUcqguDdE0hDN7PqJ+kjo1ncA\nTOADRa4pZYfEkM9MPHituwD5yhTnVlzuQmj3OsTQEMzXEFwngXZhvSlrs4uCUlQwktXjPuIt8K6U\nYuuDBJw0HHl4BzQCD99ySklAnmwP0Q3/Zo3FI5HQ5sRL5OAvQiFVKt5yoZGQjdBUpfQe8j0MKpI3\nOZSqNa2nAMIz7twToeHCUCrNYFnS+toM55Ixz4iEyKbC60WxpTKWChFUUZDSG/m9Gnr7NgdaC7fH\nm1dy/I9U7i3K517DnWBdnMyhc366839r2Iw1V9TzUNK9r00ZSee6+zISPZf0YZ6ME8kL1gt4V3qg\n4K51fX4N5zwOD1Z6r0mO7QPcZ4KG+xFi6bkGw9tNR+E5IZbUSprWuuCECu4NZEFJTz00+w9JKB4z\nIQWPBe0WcdUJj4bJU3TcMe4njs/AfMA8Fz+8QAmKBCLGIs9QDog0AgE13I4gSmXI1+oZhnNDw7Le\nsOP9RfYU3aHxhGjZb0tVUI3s7XPhEu49wf9wERGkFmodKaWweOaAvCOKMyZRUqH3fkZZ6uJnZKbR\ny1wy7Aqea1sJaMHSW6t757G7yyZxT3G5MfeCI9GyraMR2ZPKmzG3Rlssqdk6h95Du8XChSikopVa\nOtlfBAVB68AKhJeefHY3zJWhFJSKR1A0En2nJVuKe7LV3tzeME0TKsphPpBtOgRzZxqvUhEBa3+R\nIitBZaCyWh1ASaj4q5AA5nlmGC680rzf/5W9+/mvzxrenYXoZIXC3Qvb5X99qVfp+us01huibgUt\nBKe2E3fU34mn7m4CH86D5LkYrJLQbl/Pq1e2r03utpDk2Tk+NP79umTxLxLMoG3L1RgzBYOhsdiR\norvswUYQ3LL0myq+Jp133XNNdnXCTryA4bgsKBOD7jOk2rdBFuo4JhfaLRCGRTajEwMGJXNSR5DG\n2i4EIks7Yu4+0cTG0NEbxUU40QQd9ww8oZRrBrnKRdmMDOkp8tKspa9Bwnh55PeKYJPOzkA6wJ0K\naAUciJatmB/A3De03Fa6WmKbP6KedEJ5YtydNy8+yaD/ZlIzIA51Rbz24MhihjXL/FFveaHbdH3Y\nJLoIhTSW7BWia38cJQcgADGCxN6LKRMFt1QWU61QnCpZzZ0PeaJLprrvrQ6CUSdMyXCfBqWH/AZR\nZCysWYOwmWHsLHZSaa1BA5FXk+MpdaTg3B5uX8nxPypZc3X35dwjSaWQ99vT/YSzGqFzyp67Xs/a\nYM9Z0XsZns3tMgq41jlwZ7/zY2f90fr5ueLqfHnZ1rZ7BkA/z/vXtSZ3V3X0XLL4EkU/h8vKdnJL\nkKi4kLwnWsB5toXCVEcKGdaLgLCWykcqSCLZKJkQyBzbQlFFurHnURN3ooJ4YOPCENlPx2bt+aiK\nm6GDQHsfmRaMWzSCQlDqQLZRIKmgKEQstHZEa0Yn8ILWa4o/YRjfpjCxG/Y8XW5AtVPqCPayfYZe\ngyzL3EFXD5fQzGt7W2i9fsg8DXPHu4JTpPRn3zIC4HGXPR+S3BXoZKgJcDFbiK2LbUDPFFqcKjPQ\n0hGWSnHHNHvH1WFkGAaMYJ7zOXFPzrokV11YWjCQXt5D82cXoZDUjKPNDHW3VePHiiR1xZcsUC1a\ns2zFScACQDiH4zOur3Z5Az1jnB7JwSSeFoOq4iQbMgijCM+WG/blauNo0Gm3AWHMW7qknoVer0Ky\n+NbvFXdeoJzDozlTRGegui1bFJ4FyX1hV9EToCHO9j3761sV+Pp7qzJZp0oGrBPGraz09+tOq/cV\nK/rvdKYnz23zsGTzgu4CInqYMNj2OXl0l62RFt6DANUhn9k+GkrgAlVHTq5m60pn2BjXm4CEoSRD\nSdWk5FIRNJRxeAPzJXMNcUT8SPYlGHBxkBktSh2hLYq3AS0BKpgdKaMRcqBKBdbQYkcDnjVWlJAk\nUsYgBqruGcunGMobTPUJSyfsZMkcRrbNFj4OhVdhC5GwgIeL0FGRgAetQ7HdHIvugYijttZZAtsv\nnCIWkCCH1fxa55ufIyG69K5IfeYJpYfPpbODr7kjkZLKKjy9P8+ymXBorfdDakYp8VLXfBkKiQSC\nuN2iHchgPifwiqxTqrVkaWuBcSy4CY2FINhdZauIWit2POJLA7SzAzt1UI7zzDhOmLekrwH2dcTa\nkn2QCMQEKxAW3T1tTOPE9IpyCGZLP5fLnlByplTuSNx9HT3MZWkx9If5/g5niLu4p6TWA/aFKltD\nOCoPeUwd7jz6Z9pyey/3vpMtNPucp7T+fYEXdWmipfcD45h5FRkyj4R2T7PhEkgUREcsEtRQy4gC\noxgellZxJEu9Qg+ZCvPyAUpSbhUZO7mt9iWwj3E9UnZCmWc0grYUhqkQvrB2Ao64ZagjEY2IrKcR\nUby3WUATVVdixGzAbYfur5nqE4hsvukW1GnoXUm9d0O9fKqGZT6CFF4u1pJGxErbE7EStCoqW6Vq\nB0XlM629+/Wa0gBYWV3X5paQdV1ZRNFRpgp4oegZ556sxlsqroyApNEyt4XFjXDnOC94S3CWe2Tz\n1BhYnw2zh9dgXYRC8uMtIQtSSjImrInxAGTkaqeIDAkL1UKIo2N0rIEk3FuVsIaqMg4Tt89u0vXv\nh1Kht7Do1BsiXO/2WW9kzjgOhBs+p/URQC1JJbQfXg178jQMLLZk6PGCJc7gny/6/FyEhPre7/y6\nTpi7dUts4bF18YP7OZ/85XWWfBjibQ2GREf6rb8X/ZxWz+r5/e+/7znLAJdydo6XKy2C2r0ejUbG\nmRMhajFnREEyf4QslDKgcVoiQrKn0KBPcBpDmzt0PHssSQjNj8mf1tvRa1S01GTMB1yD0FuGfdAC\ntBkRFY9GlSAxXhUPy862Ebg3VAtr4XoSFhbclHYoqFcYs9uzu9OWhSI1nw3pCf2PCcZuXhrlgWwF\nJzl7NjOidiaZX/LI+cKmaDKUer7r/TvkZJTJ7wXP0cwNbfPwzMDrIPFcTgNiaT2PFSxLKib3zCkV\nVaIGVYYc53g4/e1lrITLTB0gwrL+IAJrR8bdG+maekD01hCSGnw+LiiGSGEQZRjGjNNGLkqqgvUO\ns6wRnu4dNVsoZeTZzQe9YA187gnCEIgsxgv3RN29ohj1stx2V/iyPaRNEcXZ+55cOemRs5Z2XSGc\ngyC2OqEVhNBfezJEbsdc637CbOOwu5sXzcE8V3jbsbvXpXd+j15ce3Ze26WcKc0Vwn4GN8/UeoYK\nL1lG+TSqjjMTkj3DihYsbhGpaFGUQmjnL+tgBI9sWSFUIhYWe5reqCoaA2aGSCQjgA6ILxmOJXCd\nkzyT7KJs1hedIah7A/aYHSgrOzsVkSRDRbXnaZNiRrc5MGFLxY4FnfcII0ShRM0yDHeqBkjpz11a\n9+GXX1heOjjh5aQ/+QYntaL3/j4v3vNyq4icQDyN7Fad4dfn95UVzKUdUNF/OaeH4UvPNEV2vs77\n/4KTEEkkn/FS3IoXoZAGl2yap0pbjozTxDDuMM+mVvvpTZyVNDB7cSQuH0AIX7CjZmvwMBDlGM5y\nvKHqQCXQWimqLPOc1d+lIWVIS7050zhm4i0CtQ5bVMW1MrdXg7KbBnLSX3zLy3X53lTDc9/2JZ+E\n6PZP47TPvXWfcwDD889rZ1ToNE7P/dqGNDh9dyJHPf84ARPr6/tyzvB9Hs44bd8/v/DhmfybcW7R\nOKCDELEQzJ0l25LFREBcEsBQHe11PFD7PSuA47GADCBZ0OrRcD/0gOeAiJ1oyeJUe7KW0gm3RD1S\n9460ddwbqz0eGHhC78U7tYyNhE2YjXgb8KMiy8hQCrIIbV6QaOyGSpbHJpmoW5Lw1ldUlvFRSqnD\ny9ezWX+Y17DxRnkP20O5xdvys/V59k4snfWrWT5jlutjW+Z81lfEyybJ5qAiSKkZSdKS6GbLc1hi\nwZptJK0A7neLyUER9SRrLb6yEjxILkIhpbVTwKGWIUNsGuTilm12xaGUwkwu4BKCU1iOR4oWmjaM\npKwAw3oYb43BSo8KeUBrnu3P2xGzhbEO3LY5+8msvHXioN2qe0WFkbc3WX908bDvO9IniQAhz92a\nXu/KHRjEWmB09jbjA72X0Vacum1xAjBsamv99fNc1qbuTvvdTxOhm7K6G8bLjTr12xmXapy8pXup\nrYuVuEJ9xHwP0ahjYHHIf3Zz4h6TBep6P5KsOEjeOtVknA/d5VxpthUPq1awBYmZ6GvYeUqx5FTt\nYKMM74Q+RepAqRU4ZmRDejgpBNERtwy1RruiLYXlVgmvyRDdJH3T7jUfnz1j3F8hgyYUfKWq4W5o\n6VIl29283DqS63jnlPPITivbc3q65rV0IroRtfHSWQbZQgRrJ7DX+vXdszmFs5NB3ImSAArvdU3L\nYrRmtJ47Okc3JqdeHgeHihKlA05eYnguQiG1UjvUWzPZ2RpT3W2Jz+P8jEDQGDO51m/qMrdE0oly\nXGZQZTGjamEsAzoUfFkQEs56c3tLKVnQdbjtbSZUaYcEF7gfGbSgFVprNLsh26S/mhbjSwugJSLp\nouX585MVlQZngId7/s6GxhPohZhbVqcvii++8hcDELbfkvuT6d6uz3lOKd6Lb9eA3FrzJMjz+bCP\nAZhhFYmJ1iqqI+240I4N0QHXCjpRZME4dBj+nPkAyW1AUN33QliQaEDN2hbpzdmsIa5Eb0Mflkmp\nkBGJbPomLYN3Qha2JmDHewgW0OzGW8tERMENYi4sy4AfKtHy84hkkBYBrzUp3AKm/Q4thTqOHH2m\ntQUkqY2G8cJdWKD21MHLyJpnNUuKH5LIApBeZCugQpWCK6dWExYdzn3yrrKbbHT2hsJaTgN0o1+3\n0F4i+xyb09Py6D2XWtC8JWmqNebWeq+mgVrIEJ/IGWefdIquh1/4RSgkRmWJGSVj3eaGHw5oLRh9\n4mjGkZWKLQugVC2974ZnaG4/MX/wHqrKMi9Uj6wlQqm1Mkz7vpA6u+kKa0u2s6i5ALpEj5kr2Iyo\nUuuY7dRfgSztiMfCbv/mKzn+107uh7p6T4n1uy3fFGxhtVPsjlQJ3e3fdNiKgFsBCXo63r0HvFca\nfdmzk7U4d4WNbx7Qmdn4MVJEq4STOaJFiKVlSKwqi4+UIviY3ug47TG7zQJaMUwaHscEQ/SYW3in\nlpHerRmlMGLa7004Wq4SYNQG2qwsB8eXrAsqUpCyo5QC0hL2PVwjHEBaUgVZbtvmEV8GbJnQqMky\n0BfZBCQJJoJFoMMIArthZL49oARRgknp3GyXLVrHL/mMvkg2Zd65VWkZfoXaQSolC/c7l92mjDyR\nrg6dUopeWHQ+N8/cljVCsCqjTo7q7iwtkXVLs45otKyFWtnAna0g92Ri3p2fp+6zX14uQiG1OHZj\n25itUeuE1JI3hbwxt/N7vHH1KW4OB4qM1FIJnNoRataMw9ObTncC026fMXMKy2L/f3vnuiNJkmPn\njzQzj8is6p6ZhVYQBEHv/wx6Aj2B/gtYLYSF5tLdlZkR7makfpDmEVldO1u96tqJ1jrR1XmLu7sb\njYeH5zAcal1ioZMo90WDWec4PS0s3DqtLaz9SqnKsjR++PGP3+R919NClQXst+GHtMNxe0Po9jd3\nS+gs/yYhgBtmiFEZ7fNCPqEdcpOnwdKaNG+dg3SaO7A5GBsJLRridzCcT3HXvC13r5FQhfddjy8W\nWSUxi7vEGPO8dkeiuHXGHjlE0j1VBW0L6o3eN0pX1lWQa8FF2V6dpZ6QNqAYMkb0X3RgY6NWpXuo\nKYgovb9RqFAW1J3hAZX7usDqXF6McXW2a0Gspnq+gxbaU6W4IGY4HdET6Bs2BsWdQuN0rqwC6zUM\n4NT97tgHwUQk5LU0h6ffXl9ilrBKyIoh/AbyEafzU1Qf//Tf6dZZ1862vnG9XKMfs4F3p5vjw9Fy\nCvcBAa9A76w9LEacjpQKGs65IpNhGmzEYcbo/QZuS6rhaDAUh/c4X2JKFtQpWTsNi5kis0HvwYTs\n3egjWHVzBkoRSomKqGRFZGMOw0+RpNAqtN9ahbRd36inJSazVei20TfP1FtRhHP9QF87QsFc+HS9\n8N3TMz9++oEPH76j5wFoZUG0MLZo5C7LM0udrompuKsCFMrpiev1Aj6otUUvajOQ0I+Sopit/O7j\n33+T991Ugw31W7C8zNgJC1/q68gUxXyvkL3v9Jj9nruKZv/9becm3IPld2yIn53X89Em5PB5z+ku\nodwx8u5f+fziE05MCE/e3+Kh4yafFIk5kBphKIjVsKTwBR3C9WpIGyEf05xSHa9GUaOvIc7qYqhP\neS7wUVOzVbChbFenr05/dUL0FMaA2rJnpIp4ycF0QcZApOJDEFlBNkSMos5yqmyXzjDFNkA7WhfS\nTxMsFFZqE2xb2S5rupYa4p6blce/fqTkUL8PylBqgaEhPqtZsBhO7YNBSJlJ0ShWcSSHioFkoUZ5\n4iNKKEOyh+d0H3gy4JjD3yp0pm9WZ3hPqbQAC4YUYPyzJ/y+gYQkPeT1JuU9WWMXik75p32T+nVJ\n6SESUtWoWkQLSMHGxmALPTuU6zY4n57pG/uCcaoLazdOzx/YxoYWzV152OguS3i+u3X8bvK8FElB\nwRGNwSa4CddxBTlT6kIfzmn5ABiuJVWov8H7bie29QWVx27K3lO2I/Ys8m6wdYJvc3ZlctdufZ3U\nnpuPkRABPu93Z+Tlt7Twucvrbc7olmRmX2p6Nb3rI+2/23/Bnlfz8XdtvTtqeP7xX/eh/RvG2HpU\ngW70zdAi9IRGuwXr1A02N8QL3pPQrg5mDAUtgswBcXMohvqCZUVsJgEZ9agvjZSUcqgN5Lkme3W5\n9Qe7hmyMNWQ0VM9I2YBPwFQOuFIrMIyxtqBwG2hb7sU4EJXUOTDcaw4WKj4c+Q1IB3lWEWoFL4KE\n6hnVKj5GQKJqIAOxAWMmo3zvUmh+RjSqSRsxiLp1o49JNrj5GQXJTpFSCWuPUNQYZvS+MswpJWzR\na2sUBS0pEyQJmQsMK6gaWjXaiZ/B8zE0nz9mDwvuZ5+mYNXXXUcPkZBEa4oHxsRxLSF5PzBKC5HV\ntQeVx4fvpWK3LeaGBKbqoJa4/8vrTzwvZ0z2VZPSCqKRsDBwWWkqmEKt5/SNd7brlY/tQwzbTkz9\nG8SyLCAbRR/iMPzC+OaZbZsAACAASURBVDlWDLMa+tLJ9xnh4Wf3vxGv/QvsvS8/3qysZkL8nAjx\n7oXd/cXz/++//lbDzMIsNPZkYVpJ4PvTOiKGHiV2zADUSGJBwAvlg22wK61vcwZLWbdreBTpORxB\nh0NVvIJtFlpqEB5kEA0JT/NSAbN0F7YT4gsugvsb+EpBqGUgbaBP4YkGIFaQEpsbVDidz1z7KyaK\nurM7mA4PNteDx75A7wSCiiq0VjAdhPX4oJtEVZrVEaTEmChlMcQK2tNAxGLNG9bZ+mDdehAgjBDA\n7YKWPKa5mfcxgl3phiyn7JezbxpVY1M3rAbLnGQrm+Lm6B35556xB3EevjsSZtk7+o2pfeupsPlG\nqy36CCrUIowR3hpDNCRRKBRKskkiQa3XC+10ymHLaLyttsWgrA9KqXhfs+S1cI0tQqvCtV+RuqBF\naaXy8vKCaGVZSuDqFrv7b+WHNPorKlAe3DF2Jpn7ZPMekoPJm/as7d/Ts2cymHCdzC4qYb99rwbO\nfnGQ1ddeFU1CQpIkVMtuTX7/XLvS+O2p9/cxKyP7nFU3l4xJwJiP9a/6xP5tw8WxnMtz8nh0o9vY\nd7siJO33ZvNxK3hjI+immA9qXUAEH7D1zrI8McYIBWkROCVzykGXsDC/WYBw6+NpuscSCalWwdKB\nWVDcQlC1qCMtrt8mMZOIlyA/dItGeiruq4dRnef5FhvOxx4sB+7Ut8l9d6FUASqmHQgCl9mKWyR2\nLzfUoSL0kRs1DSq1F4VSoReMK9sY9GuPa2lOr+znfww0iwasqrVxOp84nc/RwzXDRgevFJE7fc2a\nl1CPj9mFqa/nU748L51Saw5eE9fYTkT6jfWQ1mlfrLHLQoKMwJgMEUerIn1QtLCtG6oFWzeW0xOG\nYd5Zrys1+IchX2EGfWPRwsAj++eK5D44lVD2HmNFXThXYSmFcjoB7Gq43yra0wfG6On58v9T3CeI\nL0ckt1nyjzvCg6ecTCYjn7yd+2QEE89R0R2u+pfi/rLYpft/pkL816q4x41hQc29rwCLlCCWQH6u\nNUkbd5Dr/AwcKEqVim0GgZaH4kk0kBLJSDvqyHCAYG5hfyC62yS4FmrRrKxiMzNcqLJkP+UJ44KN\nK1Je6ay0ZlDBu7Jd3ui+MQjV/TGicnbS28c2REJ26CsO/d88+s68SFhaQ5pMtIT6hRjCGtCod7xP\noZ7QCNdkhsaGQyguFF8oW6euJ7oI1zHwK3jvbFuQwXzcCc04LOeNDx9/x2k58eHpA08fPjB6WNr3\nLRAiSm4eNYR2HRItMmR0OrF5eHc9EbfVojtZIswE7dZW+op4iIQ0zCltwTTcYIuUeIc58FU0hB0Z\nMR0sHawYpVUu/YoWobUz1SV2HuJIkUhwGFIqVQlXWlG2/rbvHIsKS85O1LokzTsYJ6L1F00Z/9LY\nrFOL7ru9317MPlB8/asS8/uW/EZD2H2Mkr215zG/w6YtZ1n03pMpbhPDgsmgu4cJ/zm8L5/+r1mS\n/2ya/jdybMxs93YCQAkXVy2IBCZzYxBGf2/0TqnlNjYyD0BJ+FQEzerdVaM3SBj6xUR/bP5ifimO\nrCJIKTF3Q/QxAFQLRQMy9xzDMK24LqgtqF7o/RNjGNKME7Ber5itIc6psYtXKWGnrTfLmd9AgYTf\nDbIGRip3jYBKq456DgJrQYrtzND7TVFo1ZVgCqtQu9PM4LRQTyfq6cT6duHtpxfGtrLRKRqz0UWE\nenrifD6zLKdMMoFoqBaWRItmhevZ2kAIJp0URJWCYyWSk7vv0KKUVJTUz9XWvn4NfYiEVGRBRqo0\nSAEcLZVCNFvjHQ6aLNnsLjkqIRSF3jdGv7DUGiesGFvv1MxrSy1crhdaE9btigClKN1HqDFooBYq\nwvVywWSgpSECWnVno/zaIVX39/bY8SWYbrK6cnbBHSGo1zM5zOHZ0CK8S0Y5+IiF6ddMQqSwrqRx\nnGGxCGo00iXxbfyWOGxEpWM+6eDvV6cdupsw4iQwJLb1hS4Y91DdNzr0v2rYlpI8exUZX0MVJhJA\n/D8S+zCjqlKW5QbBYgntaXKxgtygNXe8CcuJCkVO+20RQSVsqptrVLc1YLxSClpbLKBWKEuh1Eop\n0fe1VI8enkOu15+4vv7A2F7pXHlqhVINoYP3VKiejYsYtO2b84u24H+jsEkELJLK18KcdBVRSgkH\nbNFKHaE0E1VVIgfmoeYElLZQ6oLURieWj/P3nXXd+PDplU9//jPuxvXi2Ft4Tp1OC609cTo/8fz8\nHe20oKq7T1IthZqkBiRN/pyEWMN+QlSp87orRhkxB0qZUH2sY+ZOkZuVRUhSfd0xeoiEJIOAB2iY\nbVzXC6qV0uo+P4QEvm1docZOYlhyPgzOpycEeHt7QxosS6W2iruzjhBVtbTrbdOD3pzNelimS0xE\n51g5mNPZEA/ri28R3ldMlPrg0ieefZWvj8mGu7/vjXAgiUPvFVEyrqfrq5tHMzuHVwNSmgKcN2r5\n/rCeu3S9Pf/nUNs7cVckbiz3r+v9+5sUjM+pGI8YEyaxSeWVwnrdaK3effLRu3OcmuKkzL/5oLtQ\nteLFQdN4VuNza00IPkQFySQ1K1Yi8USlJbSyUM8LtYZWXmvTcCHgu1Ild/k1N5uRmEJj7e+4fPoT\nL3/5P1xe/4SPFbGVfv3Ey6cCUnFCS00KaehpO3z7yKG7ov9MRPF9HIcJRStaKrEYFrTcoGwgkpkI\n1BawZ6tUKkjhNIxt21iWhdZCjebt9Sd++vMf2dYL59MTy/LEspxoS6HWspNVlbK/pH2+T0GH0Cvp\njsBuIRGnhlA1WyvBZ2ckalEY0f+yYHuKCmpfV8Y+REKquoA4kiyNp+UDrpksENyuoUE3BqUEBi2m\niAvVDakNbKBSOLcFJ5OOG5frJfj8ACa0ujD6ANtwCa+OvoZLZVnS4sKc1grr9sb5+fzXoaj/h1Cp\nVC20B+9TvHN7/ezav9Gkb0ngnSyQ34RLPSshN8eNHS6w/D6G+sKIUZIxJtk3kgnlZdU1B3Rv2qnz\nb/EyJhHivfJ4rrLAjkXIfFMz9byvAnH5ZhXyrxVtecLHoGGgsbM9tRPmRqEgVUM6xgulOAxBahIN\nSsNmn1SV0/m0kyJqOn0GBViyHyQhVFyUbRugSlvarbeRKtGqNaDzObSpk+aSU2NplBn7EgvxZH/m\n6fkjH777D/zpf/9P3j79E9bfWF//xBgXmp5pyylgpbnRyYru0UPLkqeXJuk34FMbjpapoyqINMpk\nAu/nNrE5aCEQTVUGQikVbfGZmEPfBpfLhefvv+fj99/z45//jIjz8tNPPJ8/cFqe0VZZyoK2ul/M\n89iKzgQZ7t2mTvHK9LOCHNqdr4kKJe5vONp7GDa6MGdkxbn5Mn1FPERC8mHUU84hWZzEArRaw2Rv\nGG49lA20xaI2YF0HSytct8FSKts2WOqC0QmHWeHUYjp5qSe2tYMoo688Pz/xdn2htoJ1Y+vXOEiJ\noQ83pDS2vn6zLbJbp3unr48+2PflZARz4Q/q9V4DzYTBbQGyuejnmh87v5gBi+oIfEy1hrjRLhnk\n8U8S1pvU5Ps8MZPGe0XyWwKL16Sxo457cH9g31O/byvBhDAeOT58991O6pBawreIgLS9O+3c9l6O\naN39wFRDJisYb6H04ChFw3K7liVZcrG9qKr7Z+7AsgQE6JR9bqWohBgrhngsdM7AS0koJ3pZWmsw\ntfCwwWCSz4TTU+Xv/uN/5S+68PLDP7JuPyDrG1f/xLDvKOUpFR0q+PaNhjJ+3SgaleIwwnNoahkU\n2ze8gV7HcKrWhNA0BaYroc5QBFRj4daw3WmnUwgK1RGQa4lzYOtXzn95wvrG6bSwLAullkhkSabw\nKa6Q6tMzeVja8kyEQhhftKuA9wlnXm9F56VeUfOvXkIfIiGd6om+vtBKZWknqIW36xtCwTCezmeu\n1yvWBz4663rFDc7nZzbbEN0YRdASDdeWDdDiipagf6sWpIZkxvJ8YjDQEqQGaZHpL9cXTqdznBgS\neLiZcFkv3+aN24ZtUZE9ckzY7R3Mcx+TbuzTwGv2hVImyADLe5lHRRSiAORURlQ9opQS/aKoiiQZ\nVCFXEv2N2HHrrFrvFJRF7k58yf7Pz/pEk1Tx7h1+6V3lwOznSuSPF//pv/xnYO9IRDVqoWIwdsPC\nSD4ujo1JwL8thsOjb1RF6dsWfYO2xLjPXngKPgytZYdkZcI590PNcmOGefaARVPKM/tQbhZKKPOz\nzQ1L1YIBTx++47Qs/PD8xI9//Af6+gN9u3B9+zNFfqS0J5an37GOvldhjxxSGng4Y3tm36kHJzLl\ntAY9iQLPp4XldEqKdg2GcA2oeUheY1I4nc608wlHGcPRulGuJTYV6xufvn/C2Ti1Z5ba0FYpNESU\nMWTXAZzOyDb6bTtmfmeNU1AbWDpqR+QQ7t31cW+lE+fMDQ78mniIhGR9iwVm2xgeXi1VG2i4wV5e\nf7phmwgfnr8LX6R0oqyL7npK3kcYkWXPyIdTW8VswxmowpoJ5pzwRMBBlUXjtUgpYZVcK0utjPpt\ntsjFQaQ8eAcp410f6e7zyHmiyROIHhCZjHJuaWfQyc6ceyf64HGV3oC+e/JCJjScEG3PBW6viPIV\nCSCKpHTOfIlfoi3c4vMO0a4Nsb+XR09GAMvpnExDiwpkGG+XK8u5UHNY9d6oY0oDRk9J2EY0oQvC\n8Oh3NErAlRps1Xk8dCk73HSrHoMxNgWfZo/BJGFbC4XwUgMmEvG0NDA2s2TVem5WcsAdoBQ+fvwd\neOfHP274MMRfAWeMC2+vhkjFHmMZ++tx7z6cA6Oag8cwGZKKph6dqu7qGW0JpMiF7N2FUsPYy9WE\nHcQoArUIVgrLsvD8/IxZp0qjlIT8Uj3BHagxXByjE77rfCKk8+yt32Ulk9JsM433yYh9dglmslLN\n2aSvXOQe4kiWEVCDejBItpEDrZJ2uERzr+QumhyMVXJXqC0zs6NVktFxg21CPkVoORvR8k+lVNb+\num/el7awyUotjdpO9HGl1LYP5f3aYZcLz8uHx1dquMfq/AbDvVv5mTActxlT9yQs+J5UfNqHzGuI\nnEea398/ZT6OzF22lIRqkuFFwnu7Iyy3C+iu0X0vDfRF3PHdM9/e52ffPHSEXll8Jory9N3z3TBz\n6DM6HTO5G+eKz62p3ySC3GIUUuwGz9kd9Dr8Dra79QXBME1tuTFSOR9kklHM6evltlGQWMxs65hv\nCJoqAzEjKKkmsG0hgvz0/d+jS+X6Y8e8497xkRYLNB49hIpLmBR2s/A10Jzb8ujV1bTrCC+2sJho\nEvBdazVEWElpJ5EwcPdB3zZQTar1YHqJtdp4ev4QCgpOkoYk1BZSMULFsEL0FSFg3Mwec12c2nVK\nVtfuaTESyVRl+izdJaSsvGsNosbXmhM+xEpYTBEjHF81kke3K14KgzVmUnZGVGGzQZWKCLTyxLCO\nrSuiObmdW2uRBpnIhjgm0cuoJdwbu49gCJXCtq0Mz/kGejKxnet4/Wb2E8/t7zhzfnjIDia+7bck\nQi6Ce5WRfSK7JRO//3n2jWzu1G8KDTfRILKKCpn7Ugpu4SXTWqXWFqMBOntCNwxRfpYf56BtUPjc\nbre9TzKTCBHqEXeMpoT1/trM0qOEinBLuSG3M0VWp7YZPu7m3ebn4ZmEAr6zHrv3AZgLpUSjXEtW\nuXMw0yV6TmZ0wmU0ENp4zILdoNQ5Y5ZxWTewLeSNXHDr2DaSuR00/z46W9+islo3ZOn87g/fUcrv\nETOu1xfW9RUbHWMg+ug9WLIkzWpoQsHvfBtytKEAlGAgShyLk5K3vZUku8xWNzobjKhSI6EbIkZd\nGk/nZ8Dp1yvbusVxz2tAq4BVJDnpYZgY10PMk90QkUlCEQmZqt5jVkrHz6lC7yKv1Z1Y9i/EQySk\np/Y91/6G+BYq3adKK43Vr0jutM7nZ358fUGk0rTGjlqUrV9z3mEhW6TRGATwDSiICksL5WIdIS5Y\n64lSokdkthHSUZH5a6nBcimKeeHcnr7J+x7XUPb9wLd5/F8t7s8yv+u4yKyKcsDVJJMOsZjslRG3\nBJWPwewzvYt3qSl/E1VywA1lNwET1Wymzs2H3BJTLs9mAST5F5/L73Ztnz+vvLvdw4fHvIjnHNju\ngSNzE5EbBhv7xsBmUtoLx0hivQ8Y0K1TJEeQBLaxsW0jLSJIB9EQSC0lLWBGp5WUFbKoXNUFKpgp\n/brlLFMJ6ratiBb6utKzl3LZrpSksY8xsG3lRIuekyouleX0PaU+8Xr5CbUV59tsGH/NmJujkedd\nXAJG2MBOyABco69nePp3bfQtxGzDjyguKCe0BqMZlWjBZCza3FiFtmcRCRP5HCAPhKECQqClihJG\nfJrDThrih/vr35/XAy5UBi6CFaXsbda5I0wvJlL4dRhfu2d4iISEKWo14ADYdw9mG67GGFfethe0\nPuMSH9roGzoGTUP/bow50Z9jyR5eHrMCHtdtV9wtpcQCZDlM6YLIQi0gnECEcQ24zsZGlW8DCSyX\nJz5++D0nfXTIQTKB3KpPh5tzqM2GqMVClD2jGzQn2bS9NcfzUfN4TxYesVMUpUhFU3omFDRaqhHf\nEpL6lBoKZtHtpVrCVHFpzJ6J3fWu5mvYqwvnzrNvT7nfjPL/a8bLy08Jc2VF2I3r6JBN82FG3wb4\noG+OjZWpGTwsmuhj9ZTQyoXICahcSi6GluzXWFnck9lYAsJRKVQtjLkjjiZQMOhqbiCSxGIjkqcV\niQqoNtgGm1sa2RnDoq+stbGZc710mnrIIbngRTmdC4zOdf3xb/jpf1243w2H5inVmeyevUjKP8Xt\nhjn0qDbr3ETcY55Zm+zVscXocCSuOJNVNG3kif68EdJAEoxIoWSSyefVSTK6q3r81oW0u/ZF5K5s\no5B7kPnqp8xdD17A18ZDJCTVhaYKXth6JBETY3l6Zu2f0OL41rHquK0MrTsGjQ3iY5Vgf6gycjdV\nT08EA7y/Y3+AYsPodg1Jk9LiZOn5sVtIrfTrFV0WRv82O7CFBbqEF9QDxw1yu/8FTGUFSyhu/pOZ\nfJwdJtqrlNta/8+Gak75lxpSJUlVDQrsVM7QYCfJFFm92TGbj6CTyy29TNa55cUueVuZygap+TYv\nQr//+uBF0j/94z+AOUWFsaWci+dc1wjDNmEat82FMZK59RhK1doQD7KBSji2ughKxwg9M9ONVmrY\nh7vSTlG51FKz2T53/o5qCLSKCzbg6XxmGyO8v4ygmA8L7ToHL9HrN4uqSstC9EPeE0/UaxZgQR+v\npfF0/sPf5HP/JTF2k8qwjwC777hEqkmBXFEo3ZESkJ1ah85dxrpnCExPJN8JBjo8+u+ZdEpqeEYr\nNs6NyXKU/NskjdkYoai+XykA4UgbHbB8Dm6jFpYDzuqefI2YrDbYRV6/Nh4iIY3LFkehV2TryCiw\nGWM1yrKkNEkMj7lrDMHmpHg01zbUFKEyRmeUHAikQA1/JVxo5YRbSPNTjNJO+9zLGD0H+2IKOtQc\ne2b6wf/4838jDKxSDDUbw906tTYEp3gLTPW6IVdDrTCuxvXSsUuncoIeQ7tPT1Htjb4yxtc1/P5m\ncU9EuGvUTILC7Wt8P2nf7xhuN1Rih+48JYX8bjcH0TMqJZQ6Sg25mVq/dKrGXEwMeNb9sQWwuQGx\nvJjmtZwZZlZFt6FfySrqffb56yy9x4i3l1dQYb1cWSZ8htGvVwzZE6+IBMziUd0GuVGAymqdKgVK\nAU/nV7v7HAGRGgur6i6AK9oYLrQSxpltWlAUSSi1oMA6Ot09hihLLNDqYXMuahQtrGunFUVS0X+s\nA9Rj2FYL5RTJMOS/lKWm1uRvQKlhQmj3Jor5F+BuFoi4joYOGEIxZyhYieQrLpGPgi/Ou53iHfdK\n8jHLvqGLxDSr1L3wzyqnz2s6BgLJqYw9VORdAg2iQ+Fee9rSY8smp+xfcVgeIiEhhOaVG9IL47pC\nvTWV+wi4oKnm8N6VYQG9IQM0FrFhTl/XWfCGd4sNanviuq3IFvcpKozhiAXn3qjRg/LYt4gLpeXA\nXyYr73HhtLqgUvdFWFjYXh0ZwnYdcAXZgFVQ8/B3KQ3jzMu6craKFtjWHhIf7g+/6NlsyQF7Jp4J\naP9+kgjYlRhU5wVzW9jv+0iebEmf5UsumppSNLU1aq1RLelU9b7Ba5IDMiFJU1NZOGdrjGzCx41D\n3SSgJcd2eGvaJswLaEKIv6XoHj45pTY2bjtxPS+5abv1iyJhK1LDJ8kkgKAySlaIQf0uotQaSWbM\neSUhVRg0j1WNwdrZlM/BTdFYrPCc/rcU9gSm7pyqgG9sq1GrAJ1zK2FL7rFgSi3QB65p5udZ1U41\n1WzQbw+vBTljDsNO6r1GYpjVC5AzEyRg+v7u8zqZ77eUPA75Y67mTsgq4YW6NNwHSzsxFsvHT826\nKfiqjoqHIZ+TpAjZ+7XxmqM3WGTCeQnTSSwCu73GrOK+HqV7Fw+RkMQDa0SU8/NHVITXywt921hG\nY3u9IjooG5gONlZOH868vP7A+eP3uK2sxGfdliUhvw23nsfvjaW2aOoZIVNiG1oqrRa2rYOF3IWn\nuvTaO87gDx//wJ9/euH7Dx/xNL/qedFcr0bvyttfLoxXWGRBOmzX0AtDoJwKrTRKO2H9jU/rhbO2\ntHiuDHeuff1bH4K/Gt7n2RXL9Z5TcoGboxX3CQqyApIp7+Pzv32jMeEd0Ti9JaG3tiy01vaExM7m\nM8YYTF8jyfvEJHkkEklDuug1WcqaxKCtyl1lhDFZT3JX/dnceN7e/Tf8ZH+d6N1TSDWgzFJCxaSU\ngLH3+RUBGYKYhxllJpdShCGWFH3BWImlIQgMoxteoooRQkYoWHiZpDTgnJJaMSqKjUFtBSdN9gAx\n57puLLlIug8kKyXL8yHmB51thEq1V4m/X1LioDtNleGOu+IayhKPH9mT1BL25Cqx2UpNuSCKGG4j\n5if7oHhAloJE7wfH9U4RZQwo5c676BZDQukGgvDYx5ZJ/ZaQ+tax7pgYUh11sM33TV0Rif6ThMwU\ns1ebcwNO6n/2TjQkS3a1BikwhGMUvv4APUZCGpbJWsL9EBAKY9sobeEkjbGurNcr2oRWKtY7p3rG\nXzdCYLGjRRnbG0OM2pTaarjOSkHUEIJ5JSMYJj56DP5ZnADSNawuxGlSQSvX15Wn5SPWFRsBdVxe\nN8aobBdnu6xsn5xC4S3bEULjsm0MH3xYnnm7GPgbDFAaRSpXc8QGvrFL/D9q3MSUJ3R36yHtfaWc\nN9p3cbe7vP/Wbw3X3fo854pqrUgptNZoy2mn5O/N4LlDFrndLyu2G7NoRizOe8LZfze/SmbG90i+\niu6P81ugfAO00rhXwh7DYiDWQ4T0frdqZIWUC6BZMFF3Wi9KKU/0HHWYzW83DVHVEUOboWAzs3eo\ne4vGsQ9UryJZfeqszBxOS8vWvMYszrLk8qWpqRcSRKfWYtdthveN7bKmhloIIxVxxtgS6/pNZKQI\nASmalUZWH5q6dDpwC9+hSb3XrFTgVk1JPk4hUbrcgIV1RKwllWhnVFV6jc1cLeWO+Sps24YP57oq\nupZMQoNhdzJTmhYjEhWSv2uqxjkTozL5akZeh247AeKX4A0PkZA+PD3x8nYJGCF3u1KF70+/B3eu\nrxcYleox2/z2w4+0c0GagW7hsVJXOhvanmhtwUdnbCs0BQ1/o1ZP4Z8zOupEFbNCGxW8MgRGqBzG\nkJ7HSdGjdKJfNq6XTl+hd8O2gUpl+9R5/nDCKXiWrCUHyS7riBMLBU8Gyxa6UJfLK797PtEePSH1\nn9ffn6/V7r7bTNzIAvOvN+huh+gmBj5hOi27TEpbQndLU8cLS5YXsveZZjLaG6s2yQozSfk+bHvD\n5HbFNPbL5N3Vcg98+/7v4ROThsEEIjuT0B18pCBxjYQz7SNEkkEgQqkLZoNhQkEZ1ila0zdso5Ul\nzTEDNtdyioHOHIKNwnKCfVPHLqFxYO0W14LHUGXVMGQcrrA41SUtY5zi05QudQypobyyXekvn1i3\nldIghjGjF4XGcz96eMrwhH1HSeHgyDBzQxaw6BZrCI74dHmNz3bafQhhLSIUCoMxoNXQDWx1SvUI\nPgohRxSw6tIWhltYTphTa8NG2EtMeN1U0JGCuMTjIDePJLsbFQhYQ3aEw92gBDswoNe0NP8FTNWH\nSEiNSiVmia7J5OjeMXds2+hrzJO0emb0jaf6HW9/+cTYrixnxUtHT6kAvAjGoKtAMVw3kAsURRah\nLY2qJ2ot1Hpmu16TElvpr1d++umV1ZxLN9Q0d26zj5IQiMdC6r7Qh/H8/e8Y5kkhBtx5e7tQSqWd\nGpfXN56fvqf3DamFfunIc+NUCy+vK9j1b/jp/8vhY/Z4uMFxd6XPu03TXOzv1nXfF/bbbUQzuWRl\nVGqhtJa+MEFlnUnGplXFu7ivwu7kim5Pmqvy3XNCPrYkIykdMuc7ytcqCd9Nwsuj5yOwhLpuu2cc\npIQ6yT7ITKicRDJJ6GUMWmvYSPitTWlWZWkLJYcavYXRpahSpFCXmpBp7N5LEUpp1DaJy4VWKh9z\nISul7NIyMSuzUcspc9lNM01SuibEBKJasuvgj//4v7hcf4rKgpIN9BZN9S9AVo8W18sFKUItBU4N\nXKkGVhUf5aa7RPTFA7mp+dnEMSxJJJisuDiugkggPUWzPyVRcVlxhsV1VkthnAY2jNE3hjutxzW2\nnBe29cR6OrGuK/0ag8ljhNtrXSrLcmaMGFoe49bdin2nBrQ7cpaAeB3Web/H+4p4iIRUTye21xe2\ntwuj3haM63rF10F/WzktJzrG28tbqA5TUFl4++mN2kBX2LxCcbqF2GonJEa0OKVW2seN04dn6nJC\nh6LekG2DVRhX5/KnlddPKx1lS7JBYTB6p53PiZnCuq6YKefnE6iyrhvn8zkG+fqgm8f8hjn9Olj0\nxHaNk6DWSmmFgHpwXAAAA9JJREFUYXDtHdHC1h97xbM51b//P+NudghAckBv78/6/f3e3zxgo2RP\n1fBvmRBdmP9ln4fZb7pLLvJZMppf75pbweaekGBOyDPTUuip3aqfmXTSMn1/3aFG/jNZoQcL9Vyk\nbGCqFHfMO6XE+bpIA3W27mhL8o4bDUVaUOypsbjVVljOjes6KFJC2LMIos5yPiFSU+2kcjotWI/h\n1tpqJL9akigSQ8xFK+Y3GSJRDSZs9nOX1pg265pW6p5VbGi6CV0u6KnCFVQqw9bYOGS/kPWxjw8E\nGqIq2LKABhGnl0LtlU4wReM0HTukiTqoUiVls+qc8YoNcShqO2Y1qd0GGKJL9O/ckQ2sOpHEC6Mb\nXZViRq2xketL5Xw6cz2tXF9fucgVfwO3FVOn1cbp/MQYhvYr2xYWGsMMxZJjYfkvmZSiaPVfTG54\niIQk5xNSG947Lz+8hELGZtAF36I5OhDqU+P83e8ZvaPlhIyV7iXmK17DFKoPpwvIcsNTg5pYqeXv\ngMa6CqfTib4CfWH8+MrrDy9cXteA4zQkTmo1dDmjCNtqlFbYrPPx998DQu+Dmhj5uL6ErA3gbNRA\n6HIllsB5Ubx3elqk0zfq88L4RgaAv1bcZHc+2+580d7757/7YoUh3IZcs7k+v87H2KuTryhR7vs+\ncvfz/RPOxDJ3mO5Tl22SNWZpMSu8W4J75Hj+/mO4sebP8+WrZLKfGzgtuXBlxZQLXisFW6Oi0uJ0\n89Qf09unlojnPGaQIJ1+3F+H3kEzkv46GFQJEgO5JSh1oXDTZNulhTReuLjmLFOqSwClFnxcAmks\nSjHF2OK+j18gsW1XtFZkFMrsrYiGvp3H+1S4LeDRMEuoLqSE5kB4EY1jobNGulfl9lRoD99f1biF\npoizamjdmWsw6gyKlV242MegbwPZJJgRw5ESFPuhIellnkO7NuHSL7AcFdTCU+mX+IPIw+PjRxxx\nxBFH/LuI38De4ogjjjjiiH8PcSSkI4444ogjHiKOhHTEEUccccRDxJGQjjjiiCOOeIg4EtIRRxxx\nxBEPEUdCOuKII4444iHiSEhHHHHEEUc8RBwJ6YgjjjjiiIeIIyEdccQRRxzxEHEkpCOOOOKIIx4i\njoR0xBFHHHHEQ8SRkI444ogjjniIOBLSEUccccQRDxFHQjriiCOOOOIh4khIRxxxxBFHPEQcCemI\nI4444oiHiCMhHXHEEUcc8RBxJKQjjjjiiCMeIo6EdMQRRxxxxEPEkZCOOOKII454iDgS0hFHHHHE\nEQ8RR0I64ogjjjjiIeJISEccccQRRzxE/F/+QOYJ/2HlvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f6dc141160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagesToShow=4\n",
    "\n",
    "def flaotTensorToImage(img, mean=0, std=1):\n",
    "        \"\"\"convert a tensor to an image\"\"\"\n",
    "        img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "        img = (img*std+ mean)*255\n",
    "        img = img.astype(np.uint8)    \n",
    "        return img    \n",
    "\n",
    "for i, data in enumerate(t_loader, 0):\n",
    "    print('i=%d: '%(i))            \n",
    "    images, labels = data            \n",
    "    num = len(images)\n",
    "    \n",
    "    ax = plt.subplot(1, imagesToShow, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    for n in range(num):\n",
    "        image=images[n]\n",
    "        label=labels[n]\n",
    "        plt.imshow (flaotTensorToImage(image))\n",
    "        \n",
    "    if i==imagesToShow-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d966c8a9-d189-4b76-8def-6180f9498154",
    "_uuid": "fde155bdd9ce81e2598146c263cedfa65eaba806"
   },
   "source": [
    "## Define the model\n",
    "- A simple CNN with great performance (95% accuracy) \n",
    "- In PyTorch, a model is defined by a subclass of nn.Module. It has two methods:\n",
    "\n",
    "`__init__:` constructor. Create layers here. Note that we don't define the connections between layers in this function.\n",
    "\n",
    "`forward(x):` forward function. Receives an input variable x. Returns a output variable. Note that we actually connect the layers here dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f2eb7b14-c63b-4fdf-8370-baabd48a9943",
    "_uuid": "29184efaeb75c7105b9f144550b68814c577534a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (cnn1): ConvCNN(\n",
      "    (math): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU(0.01)\n",
      "      (3): MaxPool2d(kernel_size=(4, 4), stride=(4, 4), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (cnn2): ConvCNN(\n",
      "    (math): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU(0.01)\n",
      "      (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (cnn3): ConvCNN(\n",
      "    (math): Sequential(\n",
      "      (0): Conv2d(64, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): LeakyReLU(0.01)\n",
      "      (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (res1): ConvRes(\n",
      "    (math): Sequential(\n",
      "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): Dropout(p=0.3)\n",
      "      (2): Conv2d(512, 64, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2))\n",
      "      (3): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): ConvCNN(\n",
      "      (math): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU(0.01)\n",
      "        (3): MaxPool2d(kernel_size=(4, 4), stride=(4, 4), dilation=(1, 1), ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (1): Dropout(p=0.6)\n",
      "    (2): ConvCNN(\n",
      "      (math): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU(0.01)\n",
      "        (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (3): ConvCNN(\n",
      "      (math): Sequential(\n",
      "        (0): Conv2d(64, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): LeakyReLU(0.01)\n",
      "        (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (4): ConvRes(\n",
      "      (math): Sequential(\n",
      "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (1): Dropout(p=0.3)\n",
      "        (2): Conv2d(512, 64, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2))\n",
      "        (3): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=12, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.60)\n",
    "relu=torch.nn.LeakyReLU()\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "class ConvRes(nn.Module):\n",
    "    def __init__(self,insize, outsize):\n",
    "        super(ConvRes, self).__init__()\n",
    "        drate = .3\n",
    "        self.math = nn.Sequential(\n",
    "                 nn.BatchNorm2d(insize),\n",
    "                 nn.Dropout(drate),\n",
    "                 torch.nn.Conv2d(insize, outsize, kernel_size=2,padding=2),\n",
    "                 nn.PReLU(),\n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.math(x) \n",
    "\n",
    "class ConvCNN(nn.Module):\n",
    "    def __init__(self,insize, outsize, kernel_size=7, padding=2, pool=2, avg=True):\n",
    "        super(ConvCNN, self).__init__()\n",
    "        self.avg=avg\n",
    "        self.math = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=kernel_size,padding=padding),\n",
    "            torch.nn.BatchNorm2d(outsize),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(pool,pool),\n",
    "        )\n",
    "        self.avgpool=torch.nn.AvgPool2d(pool,pool)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.math(x)\n",
    "        if self.avg is True:\n",
    "            x=self.avgpool(x)\n",
    "        return x   \n",
    "        \n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.cnn1 = ConvCNN (3,64,  kernel_size=7, pool=4, avg=False)\n",
    "        self.cnn2 = ConvCNN (64,64, kernel_size=5, pool=2, avg=True)\n",
    "        self.cnn3 = ConvCNN (64,512, kernel_size=5, pool=2, avg=True)\n",
    "        \n",
    "        self.res1 = ConvRes (512,64)\n",
    "        \n",
    "        self.features = nn.Sequential( \n",
    "            self.cnn1,dropout,          \n",
    "            self.cnn2,\n",
    "            self.cnn3,\n",
    "            self.res1,\n",
    "        )        \n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(2304, len(classes)),             \n",
    "        )\n",
    "#         self.sig=nn.Sigmoid()        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x) \n",
    "        x = x.view(x.size(0), -1)        \n",
    "#         print (x.data.shape)\n",
    "        x = self.classifier(x)                \n",
    "#         x = self.sig(x)\n",
    "        return x        \n",
    "\n",
    "model = SimpleNet()\n",
    "# model = senetXX_generic(1, 3, 16)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr= 0.00005 * 2 * 2\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "try:\n",
    "    from pycrayon import CrayonClient\n",
    "except ImportError:\n",
    "    CrayonClient = None\n",
    "\n",
    "# tensorboad\n",
    "use_tensorboard = False\n",
    "# use_tensorboard = True and CrayonClient is not None\n",
    "\n",
    "if use_tensorboard == True:\n",
    "    cc = CrayonClient(hostname='http://192.168.1.2') # point to where you installed Crayon\n",
    "#     cc.remove_all_experiments()\n",
    "    \n",
    "model_name = (type(model).__name__)\n",
    "exp_name = datetime.datetime.now().strftime(model_name + '_' + 'bone' + '_%Y-%m-%d_%H-%M-%S')\n",
    "if use_tensorboard == True:\n",
    "    exp = cc.create_experiment(exp_name)    \n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy2(y_pred, y_actual, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = y_actual.size(0)\n",
    "\n",
    "    _, pred = y_pred.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    '''make dir if not exist'''\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "class RecorderMeter(object):\n",
    "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
    "\n",
    "    def __init__(self, total_epoch):\n",
    "        self.reset(total_epoch)\n",
    "\n",
    "    def reset(self, total_epoch):\n",
    "        assert total_epoch > 0\n",
    "        self.total_epoch = total_epoch\n",
    "        self.current_epoch = 0\n",
    "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "        self.epoch_losses = self.epoch_losses - 1\n",
    "\n",
    "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "        self.epoch_accuracy = self.epoch_accuracy\n",
    "\n",
    "    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n",
    "        assert idx >= 0 and idx < self.total_epoch, 'total_epoch : {} , but update with the {} index'.format(\n",
    "            self.total_epoch, idx)\n",
    "        self.epoch_losses[idx, 0] = train_loss\n",
    "        self.epoch_losses[idx, 1] = val_loss\n",
    "        self.epoch_accuracy[idx, 0] = train_acc\n",
    "        self.epoch_accuracy[idx, 1] = val_acc\n",
    "        self.current_epoch = idx + 1\n",
    "        return self.max_accuracy(False) == val_acc\n",
    "\n",
    "    def max_accuracy(self, istrain):\n",
    "        if self.current_epoch <= 0: return 0\n",
    "        if istrain:\n",
    "            return self.epoch_accuracy[:self.current_epoch, 0].max()\n",
    "        else:\n",
    "            return self.epoch_accuracy[:self.current_epoch, 1].max()\n",
    "\n",
    "\n",
    "    def plot_curve(self, save_path, model):\n",
    "        title = 'PyTorch:' + str((type(model).__name__)).upper() + ',LR:' + str(lr) +  ',DataSet:' + str(dataset).upper() + ',' + '\\n'\\\n",
    "                + ',Params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0) + ',Seed: %.2f' % manualSeed + \\\n",
    "                \",Torch: {}\".format(torch.__version__) + \", Batch:{}\".format(batch_size)\n",
    "\n",
    "        dpi = 80\n",
    "        width, height = 1200, 800\n",
    "        legend_fontsize = 14\n",
    "        scale_distance = 48.8\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n",
    "        y_axis = np.zeros(self.total_epoch)\n",
    "\n",
    "        plt.xlim(0, self.total_epoch)\n",
    "        plt.ylim(0, 1.0)\n",
    "        interval_y = 0.05 / 3.0\n",
    "        interval_x = 1\n",
    "        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n",
    "        plt.yticks(np.arange(0, 1.0 + interval_y, interval_y))\n",
    "        plt.grid()\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.xlabel('EPOCH', fontsize=16)\n",
    "        plt.ylabel('LOSS/ACC', fontsize=16)\n",
    "\n",
    "        y_axis[:] = self.epoch_accuracy[:, 0] / 100.0\n",
    "        plt.plot(x_axis, y_axis, color='g', linestyle='-', label='tr-accuracy/100', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_accuracy[:, 1] / 100.0\n",
    "        plt.plot(x_axis, y_axis, color='y', linestyle='-', label='val-accuracy/100', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_losses[:, 0]\n",
    "        plt.plot(x_axis, y_axis, color='r', linestyle=':', label='tr-loss', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_losses[:, 1]\n",
    "        plt.plot(x_axis, y_axis, color='b', linestyle=':', label='val-loss', lw=4)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        if save_path is not None:\n",
    "            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "            # print('---- save figure {} into {}'.format(title, save_path))\n",
    "        plt.close(fig)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0cd8c571-5d93-42b6-9ef0-9c16b6d43ef7",
    "_uuid": "1257d2cc10e64019a8ca94c814d4e179a45e04cd"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "f2cd5d63-c765-476a-9258-0152d8a06360",
    "_uuid": "d5ecfa57978ed8afd52e1551f6f5688ce9e16a5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, epoch, optimizer):\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "   \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (images, target) in enumerate(train_loader): \n",
    "        correct = 0\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            images, target = images.cuda(), target.cuda()\n",
    "            images, target = Variable(images), Variable(target)\n",
    "        # compute y_pred\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec1 = accuracy2(y_pred.data, target.data, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "        \n",
    "        pred = y_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "        accuracy = 100. * correct / len(images)\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % 100  == 0:\n",
    "            print('TRAIN: LOSS-->{loss.val:.4f} ({loss.avg:.4f})\\t' 'ACC-->{acc.val:.3f}% ({acc.avg:.3f}%)'.format(loss=losses, acc=acc))\n",
    "            if use_tensorboard:\n",
    "                exp.add_scalar_value('tr_epoch_loss', losses.avg, step=epoch)\n",
    "                exp.add_scalar_value('tr_epoch_acc', acc.avg, step=epoch)\n",
    "                \n",
    "            print('TRAIN: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "                epoch, batch_idx * len(images), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                correct, len(images),\n",
    "                accuracy))            \n",
    "    \n",
    "\n",
    "    return float('{loss.avg:.4f}'.format(loss=losses)), float('{acc.avg:.4f}'.format(acc=acc))\n",
    "\n",
    "def validate(val_loader, model, epoch):\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "\n",
    "        if use_cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            images, labels = Variable(images, volatile=True), Variable(labels)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, temp_var = accuracy2(y_pred.data, labels.data, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100== 0:\n",
    "            print('VAL:   LOSS--> {loss.val:.4f} ({loss.avg:.4f})\\t''ACC-->{acc.val:.3f} ({acc.avg:.3f})'.format(loss=losses, acc=acc))\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            if use_tensorboard:\n",
    "                exp.add_scalar_value('val_epoch_loss', losses.avg, step=epoch)\n",
    "                exp.add_scalar_value('val_epoch_acc', acc.avg, step=epoch)\n",
    "\n",
    "    print(' * Accuracy {acc.avg:.4f}'.format(acc=acc))\n",
    "    return float('{loss.avg:.6f}'.format(loss=losses)), float('{acc.avg:.6f}'.format(acc=acc))\n",
    "\n",
    "test_trans = valid_trans\n",
    "test_data_dir = 'd:/db/data/seedings/test/'\n",
    "\n",
    "def testImageLoader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "#     image = Image.open(image_name)\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = test_trans(image)\n",
    "#     image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  \n",
    "    if use_cuda:\n",
    "#         print (\"cuda\")\n",
    "        image.cuda()         \n",
    "    return image  \n",
    "\n",
    "def testModel(test_dir, local_model):    \n",
    "    if use_cuda:\n",
    "        local_model.cuda()\n",
    "    \n",
    "    local_model.eval()\n",
    "    \n",
    "    columns = ['file', 'species']\n",
    "    df_pred = pd.DataFrame(data=np.zeros((0, len(columns))), columns=columns)\n",
    "#     df_pred.species.astype(int)\n",
    "    for index, row in (sample_submission.iterrows()):\n",
    "#         for file in os.listdir(test_dir):            \n",
    "        currImage=os.path.join(test_dir, row['file'])\n",
    "        if os.path.isfile(currImage):\n",
    "            X_tensor_test=testImageLoader (currImage)            \n",
    "#             print (type(X_tensor_test))\n",
    "            if use_cuda:\n",
    "                X_tensor_test = Variable(X_tensor_test.cuda()) \n",
    "            else:\n",
    "                X_tensor_test = Variable(X_tensor_test)        \n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            predicted_val = (local_model(X_tensor_test)).data.max(1)[1] # get the index of the max log-probability\n",
    "#             predicted_val = predicted_val.data.max(1, keepdim=True)[1]\n",
    "            p_test = (predicted_val.cpu().numpy().item())\n",
    "            df_pred = df_pred.append({'file': row['file'], 'species': num_to_class[int(p_test)]}, ignore_index=True)             \n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b939262-bef9-4384-9e65-1c6f19b0e7af",
    "_uuid": "c89aec6e431baa5ad878aa07c30faef161dea697"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c75d0756-757e-4cdb-b3e3-43d0ae2110eb",
    "_uuid": "56b469e006382a0c93c7ce30b7976783257762b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: 999\n",
      "python version : 3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "torch  version : 0.3.1.post2\n",
      "cudnn  version : 7003\n",
      "=> Final model name 'SimpleNet'\n",
      "MODEL: SimpleNet\n",
      "dataset: seedings\n",
      "    Total params: 1.09M\n",
      "MODEL: SimpleNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                           | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->2.4726 (2.4726)\tACC-->6.250% (6.250%)\n",
      "TRAIN: 0 [0/4038 (0%)]\tLoss: 2.472564, Accuracy: 1/16 (6.250%)\n",
      "TRAIN: LOSS-->2.0242 (2.2620)\tACC-->18.750% (20.421%)\n",
      "TRAIN: 0 [1600/4038 (40%)]\tLoss: 2.024192, Accuracy: 3/16 (18.750%)\n",
      "TRAIN: LOSS-->2.4040 (2.1687)\tACC-->25.000% (24.845%)\n",
      "TRAIN: 0 [3200/4038 (79%)]\tLoss: 2.403977, Accuracy: 4/16 (25.000%)\n",
      "VAL:   LOSS--> 1.4512 (1.4512)\tACC-->50.000 (50.000)\n",
      " * Accuracy 39.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                                                                                | 1/300 [00:56<4:42:44, 56.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->1.9909 (1.9909)\tACC-->31.250% (31.250%)\n",
      "TRAIN: 1 [0/4038 (0%)]\tLoss: 1.990873, Accuracy: 5/16 (31.250%)\n",
      "TRAIN: LOSS-->1.7062 (1.9661)\tACC-->31.250% (31.869%)\n",
      "TRAIN: 1 [1600/4038 (40%)]\tLoss: 1.706175, Accuracy: 5/16 (31.250%)\n",
      "TRAIN: LOSS-->1.9639 (1.9189)\tACC-->31.250% (34.359%)\n",
      "TRAIN: 1 [3200/4038 (79%)]\tLoss: 1.963945, Accuracy: 5/16 (31.250%)\n",
      "VAL:   LOSS--> 1.5819 (1.5819)\tACC-->50.000 (50.000)\n",
      " * Accuracy 35.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                                                                                                | 2/300 [01:52<4:39:42, 56.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->1.9303 (1.9303)\tACC-->43.750% (43.750%)\n",
      "TRAIN: 2 [0/4038 (0%)]\tLoss: 1.930264, Accuracy: 7/16 (43.750%)\n",
      "TRAIN: LOSS-->1.6069 (1.8077)\tACC-->43.750% (39.233%)\n",
      "TRAIN: 2 [1600/4038 (40%)]\tLoss: 1.606885, Accuracy: 7/16 (43.750%)\n",
      "TRAIN: LOSS-->1.5891 (1.7827)\tACC-->43.750% (39.863%)\n",
      "TRAIN: 2 [3200/4038 (79%)]\tLoss: 1.589079, Accuracy: 7/16 (43.750%)\n",
      "VAL:   LOSS--> 1.0351 (1.0351)\tACC-->75.000 (75.000)\n",
      " * Accuracy 61.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▍                                                                                                                                               | 3/300 [02:48<4:37:43, 56.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->1.5845 (1.5845)\tACC-->56.250% (56.250%)\n",
      "TRAIN: 3 [0/4038 (0%)]\tLoss: 1.584460, Accuracy: 9/16 (56.250%)\n",
      "TRAIN: LOSS-->1.3974 (1.6877)\tACC-->43.750% (43.502%)\n",
      "TRAIN: 3 [1600/4038 (40%)]\tLoss: 1.397392, Accuracy: 7/16 (43.750%)\n",
      "TRAIN: LOSS-->1.8838 (1.6810)\tACC-->50.000% (42.444%)\n",
      "TRAIN: 3 [3200/4038 (79%)]\tLoss: 1.883783, Accuracy: 8/16 (50.000%)\n",
      "VAL:   LOSS--> 1.0645 (1.0645)\tACC-->62.500 (62.500)\n",
      " * Accuracy 60.2528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▉                                                                                                                                               | 4/300 [03:44<4:36:45, 56.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->1.9384 (1.9384)\tACC-->37.500% (37.500%)\n",
      "TRAIN: 4 [0/4038 (0%)]\tLoss: 1.938384, Accuracy: 6/16 (37.500%)\n",
      "TRAIN: LOSS-->1.5065 (1.6174)\tACC-->31.250% (44.864%)\n",
      "TRAIN: 4 [1600/4038 (40%)]\tLoss: 1.506513, Accuracy: 5/16 (31.250%)\n",
      "TRAIN: LOSS-->1.2040 (1.6076)\tACC-->62.500% (45.647%)\n",
      "TRAIN: 4 [3200/4038 (79%)]\tLoss: 1.203999, Accuracy: 10/16 (62.500%)\n",
      "VAL:   LOSS--> 0.6919 (0.6919)\tACC-->81.250 (81.250)\n",
      " * Accuracy 68.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▍                                                                                                                                              | 5/300 [04:40<4:36:01, 56.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: LOSS-->1.5812 (1.5812)\tACC-->56.250% (56.250%)\n",
      "TRAIN: 5 [0/4038 (0%)]\tLoss: 1.581157, Accuracy: 9/16 (56.250%)\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "sample_submission.columns = ['file', 'species']\n",
    "# sample_submission['category_id'] = 0\n",
    "sample_submission.head(3)\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    epochs=300\n",
    "    runId = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')    \n",
    "    recorder = RecorderMeter(epochs)  # epoc is updated\n",
    "    model_name = (type(model).__name__)\n",
    "\n",
    "    exp_name = datetime.datetime.now().strftime(model_name + '_' + dataset + '_%Y-%m-%d_%H-%M-%S')    \n",
    "    mPath = './logs' + '/' + dataset + '/' + model_name + '/'    \n",
    "    if not os.path.isdir(mPath):\n",
    "        mkdir_p(mPath)          \n",
    "    print(\"Random Seed: {}\".format(manualSeed))\n",
    "    print(\"python version : {}\".format(sys.version.replace('\\n', ' ')))\n",
    "    print(\"torch  version : {}\".format(torch.__version__))\n",
    "    print(\"cudnn  version : {}\".format(torch.backends.cudnn.version()))    \n",
    "    print(\"=> Final model name '{}'\".format(model_name))            \n",
    "    print (\"MODEL: {}\".format( str(type(model).__name__)))\n",
    "    print (\"dataset: {}\".format(dataset))\n",
    "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))    \n",
    "    \n",
    "    print (\"MODEL: {}\".format( str(type(model).__name__)))\n",
    "    for epoch in tqdm(range(0, epochs)):        \n",
    "        train_result, accuracy_tr=train(t_loader, model, epoch, optimizer)\n",
    "        val_loss, val_accuracy= validate(v_loader, model, epoch)  \n",
    "        \n",
    "        recorder.update(epoch, train_result, accuracy_tr, val_loss, val_accuracy)               \n",
    "        recorder.plot_curve(os.path.join(mPath, model_name + '_' + runId + '.png'), model)\n",
    "        \n",
    "        if float(val_accuracy) > float(94.0):            \n",
    "            print (\"EARLY STOP\")\n",
    "            df_pred=testModel(test_data_dir,model)\n",
    "            df_pred.to_csv(str(type(model).__name__) + '_' + str(val_accuracy) + '_' + \n",
    "                           str(epoch) + \"_sub.csv\", columns=('file', 'species'), index=None)         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example submission on the Kaggle seedlings DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), str(type(model).__name__) + '_' + str(val_accuracy) + '_.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, participants will be asked to provide the following classification rates:\n",
    "\n",
    "-- TP (True Positive, which is the number of OP people correctly identified),\n",
    "\n",
    "-- FP (False Positive, which is the number of CT people incorrectly identified),\n",
    "\n",
    "-- TN (True Negative, which is the number of CT people correctly identified),\n",
    "\n",
    "-- FN (False Negative, which is the number of OP people incorrectly identified),\n",
    "\n",
    "-- Sn (True positive rate or sensitivity) as Sn = TP/(TP + FN),\n",
    "\n",
    "-- Sp (Specificity or True Negative Rate) as Sp = TN/(FP + TN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
